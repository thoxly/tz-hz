#!/usr/bin/env python3
"""–î–æ–±–∞–≤–∏—Ç—å —Å—Å—ã–ª–∫—É –≤ –ë–î —Å –æ–±—Ö–æ–¥–æ–º —Ä–µ–¥–∏—Ä–µ–∫—Ç–æ–≤."""
import requests
import asyncio
import sys
from app.crawler.parser import HTMLParser
from app.crawler.storage import Storage
from app.database.database import get_session_factory
from app.utils import extract_doc_id
from datetime import datetime

async def add_url_to_db(url):
    """–î–æ–±–∞–≤–∏—Ç—å URL –≤ –ë–î —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π —Ä–µ–¥–∏—Ä–µ–∫—Ç–æ–≤."""
    print(f"üöÄ –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å—Å—ã–ª–∫–∏: {url}")
    
    storage = Storage()
    session_factory = get_session_factory()
    parser = HTMLParser("https://elma365.com")
    
    try:
        print("‚è≥ –ó–∞–≥—Ä—É–∂–∞—é —Å—Ç—Ä–∞–Ω–∏—Ü—É...")
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º requests —Å —Ä—É—á–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π —Ä–µ–¥–∏—Ä–µ–∫—Ç–æ–≤
        session = requests.Session()
        session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'ru-RU,ru;q=0.9'
        })
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ä–µ–¥–∏—Ä–µ–∫—Ç—ã –≤—Ä—É—á–Ω—É—é
        final_url = url
        max_redirects = 5
        redirect_count = 0
        
        while redirect_count < max_redirects:
            try:
                response = session.get(final_url, timeout=30, allow_redirects=False)
                print(f"–°—Ç–∞—Ç—É—Å: {response.status_code}, URL: {final_url}")
                
                if response.status_code == 200:
                    break
                elif response.status_code in [301, 302, 303, 307, 308]:
                    redirect_url = response.headers.get('Location', '')
                    if redirect_url:
                        if redirect_url.startswith('/'):
                            from urllib.parse import urlparse
                            parsed = urlparse(final_url)
                            redirect_url = f"{parsed.scheme}://{parsed.netloc}{redirect_url}"
                        print(f"  ‚Üí –†–µ–¥–∏—Ä–µ–∫—Ç –Ω–∞: {redirect_url}")
                        final_url = redirect_url
                        redirect_count += 1
                    else:
                        break
                else:
                    break
            except Exception as e:
                print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ: {e}")
                # –ü—Ä–æ–±—É–µ–º —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º–∏ —Ä–µ–¥–∏—Ä–µ–∫—Ç–∞–º–∏
                response = session.get(final_url, timeout=30, allow_redirects=True)
                break
        
        if redirect_count >= max_redirects:
            print("‚ö† –î–æ—Å—Ç–∏–≥–Ω—É—Ç –ª–∏–º–∏—Ç —Ä–µ–¥–∏—Ä–µ–∫—Ç–æ–≤. –ò—Å–ø–æ–ª—å–∑—É—é –ø–æ—Å–ª–µ–¥–Ω–∏–π URL —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º–∏ —Ä–µ–¥–∏—Ä–µ–∫—Ç–∞–º–∏...")
            response = session.get(final_url, timeout=30, allow_redirects=True)
        
        print(f"‚úì –§–∏–Ω–∞–ª—å–Ω—ã–π —Å—Ç–∞—Ç—É—Å: {response.status_code}")
        final_url = str(response.url) if hasattr(response, 'url') else final_url
        print(f"‚úì –§–∏–Ω–∞–ª—å–Ω—ã–π URL: {final_url}")
        
        if response.status_code == 200:
            print(f"‚úì –†–∞–∑–º–µ—Ä: {len(response.text)} —Å–∏–º–≤–æ–ª–æ–≤")
            
            # –ü–∞—Ä—Å–∏–º –¥–∞–Ω–Ω—ã–µ
            parsed_data = parser.parse(response.text, final_url)
            
            doc_data = {
                'doc_id': extract_doc_id(final_url),
                'url': final_url,
                'title': parsed_data['title'] or "–°—Ç—Ä–∞–Ω–∏—Ü–∞ –ø–æ–º–æ—â–∏",
                'breadcrumbs': parsed_data['breadcrumbs'],
                'section': parsed_data['section'] or "–ü–æ–º–æ—â—å",
                'html': parsed_data['html'],
                'plain_text': parsed_data['plain_text'],
                'last_crawled': datetime.now(),
                'links': parsed_data['links'],
                'depth': 0
            }
            
            print(f"\nüìÑ –î–æ–∫—É–º–µ–Ω—Ç: {doc_data['doc_id']}")
            print(f"üìù –ó–∞–≥–æ–ª–æ–≤–æ–∫: {doc_data['title']}")
            print(f"üîó –ù–∞–π–¥–µ–Ω–æ —Å—Å—ã–ª–æ–∫: {len(doc_data['links'])}")
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –ë–î
            print("\nüíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö...")
            async with session_factory() as db_session:
                result = await storage.save(db_session, doc_data)
                await db_session.commit()
                
                print(f"‚úÖ –£—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤ –ë–î!")
                print(f"   Doc ID: {doc_data['doc_id']}")
                print(f"   URL: {doc_data['url']}")
                
                return True
        else:
            print(f"‚úó –û—à–∏–±–∫–∞: —Å—Ç–∞—Ç—É—Å {response.status_code}")
            # –í—Å–µ —Ä–∞–≤–Ω–æ —Å–æ–∑–¥–∞–¥–∏–º –∑–∞–ø–∏—Å—å —Å –±–∞–∑–æ–≤–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π
            print("–°–æ–∑–¥–∞—é –∑–∞–ø–∏—Å—å —Å –±–∞–∑–æ–≤–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π...")
            doc_data = {
                'doc_id': extract_doc_id(url),
                'url': url,
                'title': "–°—Ç—Ä–∞–Ω–∏—Ü–∞ –ø–æ–º–æ—â–∏ (–Ω–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å)",
                'breadcrumbs': [],
                'section': "–ü–æ–º–æ—â—å",
                'html': f"<html><body><p>–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É. –°—Ç–∞—Ç—É—Å: {response.status_code}</p></body></html>",
                'plain_text': f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É. –°—Ç–∞—Ç—É—Å: {response.status_code}",
                'last_crawled': datetime.now(),
                'links': [],
                'depth': 0
            }
            
            async with session_factory() as db_session:
                result = await storage.save(db_session, doc_data)
                await db_session.commit()
                print("‚úÖ –ë–∞–∑–æ–≤–∞—è –∑–∞–ø–∏—Å—å —Å–æ–∑–¥–∞–Ω–∞ –≤ –ë–î")
                return True
            
    except requests.exceptions.TooManyRedirects:
        print("‚ö† –°–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ —Ä–µ–¥–∏—Ä–µ–∫—Ç–æ–≤. –°–æ–∑–¥–∞—é –∑–∞–ø–∏—Å—å —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –ø—Ä–æ–±–ª–µ–º–µ...")
        # –°–æ–∑–¥–∞–µ–º –∑–∞–ø–∏—Å—å –æ –ø—Ä–æ–±–ª–µ–º–µ
        doc_data = {
            'doc_id': extract_doc_id(url),
            'url': url,
            'title': "–°—Ç—Ä–∞–Ω–∏—Ü–∞ –ø–æ–º–æ—â–∏ (–ø—Ä–æ–±–ª–µ–º–∞ —Å —Ä–µ–¥–∏—Ä–µ–∫—Ç–∞–º–∏)",
            'breadcrumbs': [],
            'section': "–ü–æ–º–æ—â—å",
            'html': "<html><body><p>–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É –∏–∑-–∑–∞ –±–µ—Å–∫–æ–Ω–µ—á–Ω—ã—Ö —Ä–µ–¥–∏—Ä–µ–∫—Ç–æ–≤.</p></body></html>",
            'plain_text': "–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É –∏–∑-–∑–∞ –±–µ—Å–∫–æ–Ω–µ—á–Ω—ã—Ö —Ä–µ–¥–∏—Ä–µ–∫—Ç–æ–≤.",
            'last_crawled': datetime.now(),
            'links': [],
            'depth': 0
        }
        
        async with session_factory() as db_session:
            result = await storage.save(db_session, doc_data)
            await db_session.commit()
            print("‚úÖ –ó–∞–ø–∏—Å—å —Å–æ–∑–¥–∞–Ω–∞ –≤ –ë–î (—Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –ø—Ä–æ–±–ª–µ–º–µ)")
            return True
            
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    url = "https://elma365.com/ru/help"
    if len(sys.argv) > 1:
        url = sys.argv[1]
    
    success = asyncio.run(add_url_to_db(url))
    if success:
        print("\n" + "="*60)
        print("‚úì –î–ê–ù–ù–´–ï –ó–ê–ü–ò–°–ê–ù–´ –í –ë–ê–ó–£ –î–ê–ù–ù–´–•!")
        print("="*60)
        print("–ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã:")
        print("  http://127.0.0.1:8000/api/docs")
        print("="*60)
    else:
        print("\n‚úó –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –¥–∞–Ω–Ω—ã–µ")
        sys.exit(1)

