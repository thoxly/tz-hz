#!/usr/bin/env python3
"""–ü–æ–ª—É—á–∏—Ç—å –≤—Å–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏–∑ https://elma365.com/ru/help."""
import requests
import asyncio
import sys
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
from app.crawler.parser import HTMLParser
from app.crawler.storage import Storage
from app.database.database import get_session_factory
from app.utils import extract_doc_id, normalize_url, is_valid_help_url
from datetime import datetime
from typing import Set, List

async def crawl_all_pages(start_url):
    """–ü–æ–ª—É—á–∏—Ç—å –≤—Å–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏–∑ —Ä–∞–∑–¥–µ–ª–∞ –ø–æ–º–æ—â–∏."""
    print(f"üöÄ –ù–∞—á–∏–Ω–∞—é —Å–±–æ—Ä –≤—Å–µ—Ö —Å—Ç—Ä–∞–Ω–∏—Ü –∏–∑: {start_url}")
    print("=" * 60)
    
    storage = Storage()
    session_factory = get_session_factory()
    parser = HTMLParser("https://elma365.com")
    
    # –°–æ–∑–¥–∞–µ–º —Å–µ—Å—Å–∏—é —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ –∑–∞–≥–æ–ª–æ–≤–∫–∞–º–∏
    session = requests.Session()
    session.headers.update({
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'ru-RU,ru;q=0.9,en-US;q=0.8,en;q=0.7',
        'Accept-Encoding': 'gzip, deflate, br',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1'
    })
    
    visited_urls: Set[str] = set()
    urls_to_process: List[str] = [start_url]
    base_url = "https://elma365.com"
    saved_count = 0
    
    try:
        # –ü—Ä–æ–±—É–µ–º –ø–æ–ª—É—á–∏—Ç—å –≥–ª–∞–≤–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É, –æ–±—Ö–æ–¥—è —Ä–µ–¥–∏—Ä–µ–∫—Ç—ã
        print("‚è≥ –ü–æ–ª—É—á–∞—é –≥–ª–∞–≤–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É...")
        
        # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã URL
        test_urls = [
            "https://elma365.com/ru/help/",
            "http://elma365.com/ru/help/",
            "https://elma365.com/help/",
            "https://elma365.com/ru/help"
        ]
        
        main_page_html = None
        main_page_url = None
        
        for test_url in test_urls:
            try:
                print(f"  –ü—Ä–æ–±—É—é: {test_url}")
                response = session.get(test_url, timeout=30, allow_redirects=True)
                if response.status_code == 200 and len(response.text) > 1000:
                    main_page_html = response.text
                    main_page_url = str(response.url)
                    print(f"  ‚úì –£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ: {main_page_url} ({len(main_page_html)} —Å–∏–º–≤–æ–ª–æ–≤)")
                    break
            except Exception as e:
                print(f"  ‚úó –û—à–∏–±–∫–∞: {e}")
                continue
        
        if not main_page_html:
            print("‚ö† –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –≥–ª–∞–≤–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É —á–µ—Ä–µ–∑ requests")
            print("–ü—Ä–æ–±—É—é –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å BeautifulSoup –Ω–∞–ø—Ä—è–º—É—é...")
            
            # –ü–æ–ø—Ä–æ–±—É–µ–º —á–µ—Ä–µ–∑ –¥—Ä—É–≥–æ–π –º–µ—Ç–æ–¥
            try:
                response = session.get("https://elma365.com", timeout=30)
                if response.status_code == 200:
                    # –ü–æ–ø—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ —Å—Å—ã–ª–∫—É –Ω–∞ help
                    soup = BeautifulSoup(response.text, 'html.parser')
                    help_link = soup.find('a', href=lambda x: x and '/help' in x)
                    if help_link:
                        help_url = urljoin("https://elma365.com", help_link['href'])
                        print(f"–ù–∞–π–¥–µ–Ω–∞ —Å—Å—ã–ª–∫–∞ –Ω–∞ help: {help_url}")
                        response = session.get(help_url, timeout=30, allow_redirects=True)
                        if response.status_code == 200:
                            main_page_html = response.text
                            main_page_url = str(response.url)
            except Exception as e:
                print(f"–û—à–∏–±–∫–∞: {e}")
        
        if not main_page_html:
            print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –≥–ª–∞–≤–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É")
            print("–°–æ–∑–¥–∞—é –∑–∞–ø–∏—Å—å —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –ø—Ä–æ–±–ª–µ–º–µ...")
            
            # –°–æ–∑–¥–∞–µ–º –±–∞–∑–æ–≤—É—é –∑–∞–ø–∏—Å—å
            doc_data = {
                'doc_id': 'help_main',
                'url': start_url,
                'title': '–ì–ª–∞–≤–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –ø–æ–º–æ—â–∏ (–Ω–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å)',
                'breadcrumbs': [],
                'section': '–ü–æ–º–æ—â—å',
                'html': '<html><body><p>–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É –∏–∑-–∑–∞ –ø—Ä–æ–±–ª–µ–º —Å —Ä–µ–¥–∏—Ä–µ–∫—Ç–∞–º–∏.</p></body></html>',
                'plain_text': '–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É –∏–∑-–∑–∞ –ø—Ä–æ–±–ª–µ–º —Å —Ä–µ–¥–∏—Ä–µ–∫—Ç–∞–º–∏.',
                'last_crawled': datetime.now(),
                'links': [],
                'depth': 0
            }
            
            async with session_factory() as db_session:
                await storage.save(db_session, doc_data)
                await db_session.commit()
                print("‚úì –ë–∞–∑–æ–≤–∞—è –∑–∞–ø–∏—Å—å —Å–æ–∑–¥–∞–Ω–∞")
            
            return False
        
        # –ü–∞—Ä—Å–∏–º –≥–ª–∞–≤–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É
        print(f"\nüìÑ –ü–∞—Ä—Å–∏–Ω–≥ –≥–ª–∞–≤–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã...")
        parsed_data = parser.parse(main_page_html, main_page_url)
        
        print(f"  –ó–∞–≥–æ–ª–æ–≤–æ–∫: {parsed_data['title']}")
        print(f"  –ù–∞–π–¥–µ–Ω–æ —Å—Å—ã–ª–æ–∫: {len(parsed_data['links'])}")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≥–ª–∞–≤–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É
        doc_data = {
            'doc_id': extract_doc_id(main_page_url),
            'url': main_page_url,
            'title': parsed_data['title'] or '–ì–ª–∞–≤–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –ø–æ–º–æ—â–∏',
            'breadcrumbs': parsed_data['breadcrumbs'],
            'section': parsed_data['section'] or '–ü–æ–º–æ—â—å',
            'html': parsed_data['html'],
            'plain_text': parsed_data['plain_text'],
            'last_crawled': datetime.now(),
            'links': parsed_data['links'],
            'depth': 0
        }
        
        async with session_factory() as db_session:
            await storage.save(db_session, doc_data)
            await db_session.commit()
            saved_count += 1
            print(f"‚úì –ì–ª–∞–≤–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {doc_data['doc_id']}")
        
        visited_urls.add(main_page_url)
        
        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —Å—Å—ã–ª–∫–∏
        all_links = set()
        for link in parsed_data['links']:
            normalized = normalize_url(link, base_url)
            if is_valid_help_url(normalized, base_url) or '/help' in normalized:
                all_links.add(normalized)
        
        print(f"\nüîó –ù–∞–π–¥–µ–Ω–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å—Å—ã–ª–æ–∫: {len(all_links)}")
        print(f"‚è≥ –ù–∞—á–∏–Ω–∞—é –æ–±—Ä–∞–±–æ—Ç–∫—É —Å—Å—ã–ª–æ–∫...\n")
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤—Å–µ –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —Å—Å—ã–ª–∫–∏
        for i, link_url in enumerate(sorted(all_links), 1):
            if link_url in visited_urls:
                continue
            
            try:
                print(f"[{i}/{len(all_links)}] –û–±—Ä–∞–±–æ—Ç–∫–∞: {link_url[:80]}...", end=" ")
                
                response = session.get(link_url, timeout=30, allow_redirects=True)
                
                if response.status_code == 200:
                    final_url = str(response.url)
                    # –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ —Ç–µ–∫—Å—Ç –ø—Ä–∞–≤–∏–ª—å–Ω–æ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω
                    response.encoding = response.apparent_encoding or 'utf-8'
                    html = response.text
                    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ–¥–∏—Ä–æ–≤–∫–∏
                    if isinstance(html, bytes):
                        html = html.decode('utf-8', errors='ignore')
                    
                    if len(html) > 500:  # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ —Ä–µ–∞–ª—å–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞
                        parsed = parser.parse(html, final_url)
                        
                        doc_data = {
                            'doc_id': extract_doc_id(final_url),
                            'url': final_url,
                            'title': parsed['title'] or '–°—Ç—Ä–∞–Ω–∏—Ü–∞ –ø–æ–º–æ—â–∏',
                            'breadcrumbs': parsed['breadcrumbs'],
                            'section': parsed['section'] or '–ü–æ–º–æ—â—å',
                            'html': parsed['html'],
                            'plain_text': parsed['plain_text'],
                            'last_crawled': datetime.now(),
                            'links': parsed['links'],
                            'depth': 1
                        }
                        
                        async with session_factory() as db_session:
                            await storage.save(db_session, doc_data)
                            await db_session.commit()
                            saved_count += 1
                        
                        visited_urls.add(final_url)
                        print("‚úì")
                    else:
                        print("‚ö† (—Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∞—è)")
                else:
                    print(f"‚úó (—Å—Ç–∞—Ç—É—Å {response.status_code})")
                    
            except Exception as e:
                print(f"‚úó ({str(e)[:50]})")
                continue
        
        print(f"\n" + "=" * 60)
        print(f"‚úÖ –ì–û–¢–û–í–û! –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {saved_count}")
        print("=" * 60)
        return True
        
    except Exception as e:
        print(f"\n‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    start_url = "https://elma365.com/ru/help"
    
    success = asyncio.run(crawl_all_pages(start_url))
    
    if success:
        print("\n‚úì –í—Å–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∑–∞–ø–∏—Å–∞–Ω—ã –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö!")
        print("–ü—Ä–æ–≤–µ—Ä—å—Ç–µ: http://127.0.0.1:8000/api/docs")
    else:
        print("\n‚ö† –ó–∞–≤–µ—Ä—à–µ–Ω–æ —Å –æ—à–∏–±–∫–∞–º–∏")
        sys.exit(1)

