{
  "doc_id": "fail-safe-kubernetes-cluster",
  "url": "https://elma365.com/ru/help/business_solutions/ /ru/help/platform/fail-safe-kubernetes-cluster.html",
  "title": "Отказоустойчивый Kubernetes-кластер",
  "breadcrumbs": [
    "ELMA365 On-Premises",
    "Подготовка инфраструктуры ELMA365 On-Premises"
  ],
  "section": "ELMA365 On-Premises > Подготовка инфраструктуры ELMA365 On-Premises | [business_solutions]",
  "html": "<!DOCTYPE html>\n<html lang=\"ru\">\n<head>\n<title>Отказоустойчивый Kubernetes-кластер</title>\n<meta content=\"Help+Manual\" name=\"generator\"/>\n<meta content=\"\" name=\"keywords\"/>\n<meta content=\"text/html; charset=utf-8\" http-equiv=\"Content-Type\"/>\n<meta content=\"IE=edge\" http-equiv=\"X-UA-Compatible\"/>\n<meta content=\"width=device-width, initial-scale=1.0\" name=\"viewport\"/>\n<meta content=\"Deckhouse — это полнофункциональная платформа на базе Open Source-компонентов, которая, кроме Kubernetes, включает дополнительные модули для мониторинга, балансировки трафика,...\" name=\"description\"/>\n<meta content=\"\" name=\"picture\"/>\n<meta content=\"website\" property=\"og:type\"/>\n<meta content=\"Cправка по Low-code платформе ELMA365\" property=\"og:title\"/>\n<meta content=\"https://elma365.com/ru/help\" property=\"og:url\"/>\n<meta content=\"\" property=\"og:image\"/>\n<meta content=\"ELMA365\" property=\"og:site_name\"/>\n<link href=\"favicon.png\" rel=\"icon\" type=\"image/png\"/>\n<link href=\"https://elma365.com/ru/help/platform/fail-safe-kubernetes-cluster.html\" rel=\"canonical\"/>\n<link href=\"https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&amp;display=swap\" rel=\"stylesheet\"/>\n<link href=\"./jquery-ui.min.css\" rel=\"stylesheet\"/>\n<link href=\"default.css\" rel=\"stylesheet\"/>\n<link href=\"./search-yandex.css\" rel=\"stylesheet\"/>\n<link href=\"./article.css\" rel=\"stylesheet\"/>\n<link href=\"./glossary.css\" rel=\"stylesheet\"/>\n<link href=\"./theme.css\" rel=\"stylesheet\"/>\n<link href=\"./tooltipster.bundle.min.css\" rel=\"stylesheet\" type=\"text/css\"/>\n<script src=\"jquery.js\" type=\"text/javascript\"></script>\n<script src=\"./tooltipster.bundle.min.js\" type=\"text/javascript\"></script>\n<script type=\"text/javascript\">\n        $(document).ready($('.tooltip').tooltipster({\n            contentCloning: true,\n            theme: 'tooltipster-borderless',\n        }));\n    </script>\n<script src=\"helpman_settings.js\" type=\"text/javascript\"></script>\n<script src=\"helpman_topicinit.js\" type=\"text/javascript\"></script>\n<script src=\"highlight.js\" type=\"text/javascript\"></script>\n<script type=\"text/javascript\">\n     $(document).ready(function(){highlight();});\n   </script>\n</head>\n<body>\n<script async=\"\" src=\"https://www.googletagmanager.com/gtag/js?id=G-M6ETBEC1R9\"></script><script>window.dataLayer=window.dataLayer || []; function gtag(){dataLayer.push(arguments);}gtag('js', new Date()); gtag('config', 'G-M6ETBEC1R9');</script>\n<script>!function(e,t,c,n,r,a,m){e.ym=e.ym||function(){(e.ym.a=e.ym.a||[]).push(arguments)},e.ym.l=1*new Date;for(var s=0;s<document.scripts.length;s++)if(document.scripts[s].src===n)return;a=t.createElement(c),m=t.getElementsByTagName(c)[0],a.async=1,a.src=n,m.parentNode.insertBefore(a,m)}(window,document,\"script\",\"https://mc.yandex.ru/metrika/tag.js\"),ym(83179930,\"init\",{clickmap:!0,trackLinks:!0,accurateTrackBounce:!0,webvisor:!0})</script><noscript><div><img alt=\"\" src=\"https://mc.yandex.ru/watch/83179930\" style=\"position:absolute;left:-9999px\"/></div></noscript> \n    \n    ﻿<header class=\"header elma-365\">\n<div class=\"container\">\n<div class=\"nav-container\">\n<div class=\"nav-wrap\">\n<a class=\"header__logo\" href=\"https://elma365.com/ru/help\">\n<img alt=\"header logo\" src=\"./logo.svg\"/>\n</a>\n<div class=\"header__navi\">\n<ul class=\"header__list\"><li><span class=\"solution-select\"><span class=\"solution-select__selected\"></span><svg fill=\"none\" height=\"4\" viewbox=\"0 0 7 4\" width=\"7\" xmlns=\"http://www.w3.org/2000/svg\"><path d=\"M1 1L3.5 3.5L6 1\" stroke=\"white\" stroke-linecap=\"round\" stroke-linejoin=\"round\"></path></svg><ul class=\"solution-select__list\"><li class=\"header__list-item\"><a class=\"project-link\" href=\"https://elma365.com/ru/help/platform/get-trial.html\">Платформа</a></li><li><a class=\"project-link\" href=\"https://elma365.com/ru/help/ecm/ecm-functions.html\">CSP</a></li><li><a class=\"project-link\" href=\"https://elma365.com/ru/help/crm/crm-functions.html\">CRM+CX</a></li><li><a class=\"project-link\" href=\"https://elma365.com/ru/help/projects/projects-functions.html\">Проекты</a></li><li><a class=\"project-link\" href=\"https://elma365.com/ru/help/service/service-functions.html\">Service</a></li><li><a class=\"project-link\" href=\"https://elma365.com/ru/help/kedo/kedo.html\">КЭДО</a></li><li><a class=\"project-link\" href=\"https://elma365.com/ru/help/business_solutions/-elma365-store.html\">Бизнес-решения</a></li><ul class=\"social-links-header\"><li class=\"social-link-header\"><a class=\"project-link\" href=\"https://elma-academy.com/ru/\" target=\"_blank\">Academy</a></li><li class=\"social-link-header\"><a class=\"project-link\" href=\"https://community.elma365.com/ru/\" target=\"_blank\">Community</a></li><li></li></ul></ul></span></li><li class=\"header__list-item\"><a class=\"header__list-item-url\" href=\"https://api.elma365.com/ru/\" target=\"_blank\">API</a></li><li class=\"header__list-item\"><a class=\"header__list-item-url\" href=\"https://tssdk.elma365.com/latest/\" target=\"_blank\">SDK</a></li><li class=\"header__list-item header__list-item-latest\"><a class=\"header__list-item-url header__list-item-last\" href=\"https://elma365.com/ru/\" target=\"_blank\">Сайт ELMA365</a></li></ul>\n<div class=\"hero-wrap\" style=\"display: flex; align-items: center;\">\n<div class=\"hero__search\" style=\"display: flex; justify-content: space-between; width: 100%; padding-left: 0;\">\n<a class=\"header__logo\" href=\"https://elma365.com/ru/help\">\n<img alt=\"header logo\" src=\"./logo.svg\"/>\n</a>\n<button class=\"hero__search-icon\" id=\"search-icon\">\n<img alt=\"search string\" src=\"search-icon-white.svg\"/>\n</button>\n<div class=\"hero__search-form\" id=\"search-panel\"><form class=\"search-form\"><label class=\"search-form__label\"><span class=\"search__icon\" id=\"reset-search\" src=\"Union.svg\"></span><input class=\"search-form__input\" type=\"text\"/></label><input class=\"search-form__submit\" type=\"submit\" value=\"Submit\"/></form></div>\n</div>\n</div>\n</div>\n</div>\n<a class=\"hero__side-icon\" href=\"#\" id=\"side-menu-icon\">\n<img alt=\"side menu\" src=\"side_menu.svg\"/>\n</a>\n</div>\n</div>\n</header>\n<main class=\"main container\">\n<aside class=\"sidebar\" id=\"sidebar\">\n<div class=\"sidebar__header\">\n<a class=\"header__logo\" href=\"https://elma365.com/ru/help\">\n<img src=\"./logo-light.svg\"/>\n</a>\n<span class=\"sidebar__close elma-365-close\" id=\"close\"></span>\n</div>\n<div class=\"sidebar__wrapper\" id=\"side-menu\">\n</div>\n</aside>\n<article class=\"article\" id=\"article\">\n<div class=\"article-inner\">\n<div class=\"content\">\n<header class=\"article__header\">\n<div class=\"article__bread\" style=\"display:flex; gap:8px;\">\n<span class=\"search-res__item-category search-res__item-category_subcategory subcategory article__badge\" id=\"subcategory\"></span>\n<nav class=\"topic__breadcrumbs\">\n<p><a href=\"elma365-on-premises.html\">ELMA365 On-Premises</a> &gt; <a href=\"infrastructure-preparation.html\">Подготовка инфраструктуры ELMA365 On-Premises</a> &gt; Kubernetes / Отказоустойчивый Kubernetes-кластер</p>\n</nav>\n</div>\n<div class=\"topic__title\"><h1 class=\"p_Heading1\"><span class=\"f_Heading1\">Отказоустойчивый Kubernetes-кластер</span></h1>\n</div>\n</header>\n<section class=\"article__content\">\n<div class=\"scroll-top-inner\">\n<a class=\"scroll-top\" href=\"#h1-article\"></a>\n</div>\n<!-- Placeholder for topic body. -->\n<p class=\"p_Normal\"><a class=\"weblink\" href=\"https://deckhouse.ru/\" target=\"_blank\">Deckhouse</a> — это полнофункциональная платформа на базе Open Source-компонентов, которая, кроме Kubernetes, включает дополнительные модули для мониторинга, балансировки трафика, автомасштабирования, безопасного доступа и не только. Модули преднастроены, интегрированы друг с другом и готовы к работе. Управление всеми компонентами кластера и платформы, а также их обновление полностью автоматизированы. </p>\n<p class=\"p_Normal\">Deckhouse <a class=\"weblink\" href=\"https://reestr.digital.gov.ru/reestr/490559/\" target=\"_blank\">в реестре российского ПО</a> и <a class=\"weblink\" href=\"https://landscape.cncf.io/?view-mode=card&amp;classify=category&amp;sort-by=name&amp;sort-direction=asc&amp;item=platform--certified-kubernetes-distribution--flant-deckhouse#app-definition-and-development--application-definition-image-build%20\" target=\"_blank\">сертифицирован в CNCF</a>.</p>\n<p class=\"p_Normal\">Установка состоит из следующих этапов:</p>\n<ol style=\"list-style-type:decimal\">\n<li class=\"p_Normal\" value=\"1\"><a class=\"topiclink\" href=\"fail-safe-kubernetes-cluster.html#cluster-architecture\">Архитектура кластера Kubernetes</a>.</li><li class=\"p_Normal\" value=\"2\"><a class=\"topiclink\" href=\"fail-safe-kubernetes-cluster.html#system-requirements\">Рекомендуемые системные требования</a>.</li><li class=\"p_Normal\" value=\"3\"><a class=\"topiclink\" href=\"fail-safe-kubernetes-cluster.html#preparation-config-file\">Подготовка конфигурационного файла</a>.</li><li class=\"p_Normal\" value=\"4\"><a class=\"topiclink\" href=\"fail-safe-kubernetes-cluster.html#installation\">Установка Kubernetes-кластера на базе Deckhouse</a>.</li><li class=\"p_Normal\" value=\"5\"><a class=\"topiclink\" href=\"fail-safe-kubernetes-cluster.html#addition-frontend-nodes\">Добавление frontend-узлов</a>.</li><li class=\"p_Normal\" value=\"6\"><a class=\"topiclink\" href=\"fail-safe-kubernetes-cluster.html#addition-system-nodes\">Добавление system-узлов</a>.</li><li class=\"p_Normal\" value=\"7\"><a class=\"topiclink\" href=\"fail-safe-kubernetes-cluster.html#addition-worke-nodes\">Добавление worker-узлов</a>.</li><li class=\"p_Normal\" value=\"8\"><a class=\"topiclink\" href=\"fail-safe-kubernetes-cluster.html#addition-master-nodes\">Добавление master-узлов</a>.</li><li class=\"p_Normal\" value=\"9\"><a class=\"topiclink\" href=\"fail-safe-kubernetes-cluster.html#addition-local-path-provisioner\">Добавление Local Path Provisioner</a>.</li><li class=\"p_Normal\" value=\"10\"><a class=\"topiclink\" href=\"fail-safe-kubernetes-cluster.html#installation-helm\">Установка HELM</a>.</li><li class=\"p_Normal\" value=\"11\"><a class=\"topiclink\" href=\"fail-safe-kubernetes-cluster.html#addition-balancer\">Добавление балансировщика OpenELB - VIP</a>.</li><li class=\"p_Normal\" value=\"12\"><a class=\"topiclink\" href=\"fail-safe-kubernetes-cluster.html#addition-ingress-nginx-controller\">Добавление Ingress Nginx Controller - LoadBalancer</a>.</li><li class=\"p_Normal\" value=\"13\"><a class=\"topiclink\" href=\"fail-safe-kubernetes-cluster.html#addition-user\">Добавление пользователя для доступа к веб-интерфейсу кластера</a>.</li><li class=\"p_Normal\" value=\"14\"><a class=\"topiclink\" href=\"fail-safe-kubernetes-cluster.html#load-privileges\">Привилегии запускаемых нагрузок</a>.</li></ol>\n<h2 class=\"p_Heading2\"><a class=\"hmanchor\" id=\"cluster-architecture\"></a><span class=\"f_Heading2\">Шаг 1. Архитектура кластера Kubernetes</span></h2>\n<p class=\"p_Normal\">В рамках статьи рассматривается внедрение инфраструктуры отказоустойчивого кластера Kubernetes на базе платформы Deckhouse.</p>\n<ol style=\"list-style-type:upper-roman\">\n<li class=\"p_Normal\" value=\"1\">Структура Kubernetes-кластера.</li></ol>\n<p class=\"p_Normal\" style=\"margin: 0 0 0 37px;\"><img alt=\"fail-safe-kubernetes-cluster-1\" height=\"450\" src=\"fail-safe-kubernetes-cluster-1.png\" style=\"margin:0;width:1042px;height:450px;border:none\" width=\"1042\"/></p>\n<p class=\"p_Normal\">Чтобы развернуть минимальную структуру Kubernetes-кластера на базе платформы Deckhouse, потребуются:</p>\n<ul style=\"list-style-type:disc\">\n<li class=\"p_Normal\">персональный компьютер;</li><li class=\"p_Normal\">master-узел — три ноды;</li><li class=\"p_Normal\">worker-узел — три ноды;</li><li class=\"p_Normal\">system-узел — две ноды;</li><li class=\"p_Normal\">frontend-узел — две ноды.</li></ul>\n<p class=\"p_Normal\">В рассматриваемом примере web‑трафик от пользователей поступает на виртуальный IP‑адрес 192.168.1.13, размещённый на frontend-нодах. Шаблон доменного имени для доступа к web-сервисам платформы Deckhouse выберите <span style=\"font-weight: bold;\">%s.example.com</span>.</p>\n<p class=\"p_Normal\">Deckhouse автоматически настраивает и управляет узлами кластера и его компонентами <span style=\"font-weight: bold;\">control plane</span>, постоянно поддерживая их актуальную конфигурацию. При развёртывании master-узлов автоматически создаются все необходимые компоненты для <span style=\"font-weight: bold;\">control plane</span> с помощью модуля <a class=\"weblink\" href=\"https://deckhouse.ru/documentation/latest/modules/040-control-plane-manager/\" target=\"_blank\">control-plane-manager</a>.</p>\n<p class=\"p_Normal\">Deckhouse создаёт сущности Kubernetes по мере необходимости и так же их удаляет. Например, если в вашем кластере нет frontend-нод и с master-нод не снято ограничение <span style=\"font-weight: bold;\">taint</span>, вы не сможете установить <span style=\"font-weight: bold;\">IngressNginxController</span>.  В кластере будут отсутствовать необходимые сущности, как <span style=\"font-weight: bold;\">ingressClass</span> и так далее. При добавлении system-нод Deckhouse автоматически развернёт компоненты мониторинга и web-сервисы для доступа к интерфейсу платформы. Web-сервисы автоматически привяжутся на <span style=\"font-weight: bold;\">%s.example.com</span>.</p>\n<ol start=\"2\" style=\"list-style-type:upper-roman\">\n<li class=\"p_Normal\" value=\"2\">Загрузка образов Deckhouse в локальный реестр образов.</li></ol>\n<p class=\"p_Normal\">Кластер Kubernetes с помощью Deckhouse можно развернуть в закрытом окружении, из которого нет доступа в интернет. Для этого предварительно скачайте на компьютере с доступом в интернет образы платформы Deckhouse и загрузите их в локальный реестр образов. Подробнее читайте в статье <a class=\"topiclink\" href=\"downloading-images-deckhouse.html\">«Загрузка образов Deckhouse»</a>.</p>\n<h2 class=\"p_Heading2\"><a class=\"hmanchor\" id=\"system-requirements\"></a><span class=\"f_Heading2\">Шаг 2. Рекомендуемые системные требования</span></h2>\n<ol style=\"list-style-type:upper-roman\">\n<li class=\"p_Normal\" value=\"1\">Персональный компьютер. <br/>\n <br/>\nКомпьютер, с которого будет производиться установка. Он нужен только для запуска инсталлятора Deckhouse и не будет частью кластера. Требования:</li></ol>\n<ul style=\"list-style-type:disc\">\n<li class=\"p_Normal\">ОС: Windows 10+, macOS 10.15+, Linux (Ubuntu 18.04+, Fedora 35+);</li><li class=\"p_Normal\">установленный docker для запуска инсталлятора Deckhouse;</li><li class=\"p_Normal\">доступ до проксирующего registry или до частного хранилища образов контейнеров с образами контейнеров Deckhouse;</li><li class=\"p_Normal\">SSH-доступ по ключу до узла, который будет master-узлом будущего кластера.</li></ul>\n<ol start=\"2\" style=\"list-style-type:upper-roman\">\n<li class=\"p_Normal\" value=\"2\">Ноды Kubernetes:</li></ol>\n<ul style=\"list-style-type:disc\">\n<li class=\"p_Normal\"><a class=\"weblink\" href=\"https://deckhouse.ru/documentation/v1/supported_versions.html\" target=\"_blank\">поддерживаемая ОС</a>;</li><li class=\"p_Normal\">конфигурация нод:</li></ul>\n<div style=\"text-align: left; text-indent: 0; padding: 0 0 0 0; margin: 0 0 0 0;\"><table style=\"border:none; border-spacing:0;\">\n<tr>\n<td style=\"vertical-align:top; padding:0; border:none\"><p class=\"p_Normal\" style=\"text-align: center;\"><span style=\"font-weight: bold;\">Наименование</span></p>\n</td>\n<td style=\"vertical-align:top; padding:0; border:none\"><p class=\"p_Normal\" style=\"text-align: center;\"><span style=\"font-weight: bold;\">vCPU</span></p>\n</td>\n<td style=\"vertical-align:top; padding:0; border:none\"><p class=\"p_Normal\" style=\"text-align: center;\"><span style=\"font-weight: bold;\">RAM (GB)</span></p>\n</td>\n<td style=\"vertical-align:top; padding:0; border:none\"><p class=\"p_Normal\" style=\"text-align: center;\"><span style=\"font-weight: bold;\">Жесткий диск (GB)</span></p>\n</td>\n<td style=\"vertical-align:top; padding:0; border:none\"><p class=\"p_Normal\" style=\"text-align: center;\"><span style=\"font-weight: bold;\">LAN (Gbit/s)</span></p>\n</td>\n</tr>\n<tr>\n<td style=\"vertical-align:top; padding:0; border:none\"><p class=\"p_Normal\" style=\"text-align: center;\">Kubernetes worker</p>\n</td>\n<td style=\"vertical-align:top; padding:0; border:none\"><p class=\"p_Normal\" style=\"text-align: center;\">8</p>\n</td>\n<td style=\"vertical-align:top; padding:0; border:none\"><p class=\"p_Normal\" style=\"text-align: center;\">16</p>\n</td>\n<td style=\"vertical-align:top; padding:0; border:none\"><p class=\"p_Normal\" style=\"text-align: center;\">60</p>\n</td>\n<td style=\"vertical-align:top; padding:0; border:none\"><p class=\"p_Normal\" style=\"text-align: center;\">1</p>\n</td>\n</tr>\n<tr>\n<td style=\"vertical-align:top; padding:0; border:none\"><p class=\"p_Normal\" style=\"text-align: center;\">Kubernetes system</p>\n</td>\n<td style=\"vertical-align:top; padding:0; border:none\"><p class=\"p_Normal\" style=\"text-align: center;\">8</p>\n</td>\n<td style=\"vertical-align:top; padding:0; border:none\"><p class=\"p_Normal\" style=\"text-align: center;\">16</p>\n</td>\n<td style=\"vertical-align:top; padding:0; border:none\"><p class=\"p_Normal\" style=\"text-align: center;\">200</p>\n</td>\n<td style=\"vertical-align:top; padding:0; border:none\"><p class=\"p_Normal\" style=\"text-align: center;\">1</p>\n</td>\n</tr>\n<tr>\n<td style=\"vertical-align:top; padding:0; border:none\"><p class=\"p_Normal\" style=\"text-align: center;\">Kubernetes master</p>\n</td>\n<td style=\"vertical-align:top; padding:0; border:none\"><p class=\"p_Normal\" style=\"text-align: center;\">4</p>\n</td>\n<td style=\"vertical-align:top; padding:0; border:none\"><p class=\"p_Normal\" style=\"text-align: center;\">8</p>\n</td>\n<td style=\"vertical-align:top; padding:0; border:none\"><p class=\"p_Normal\" style=\"text-align: center;\">60</p>\n</td>\n<td style=\"vertical-align:top; padding:0; border:none\"><p class=\"p_Normal\" style=\"text-align: center;\">1</p>\n</td>\n</tr>\n<tr>\n<td style=\"vertical-align:top; padding:0; border:none\"><p class=\"p_Normal\" style=\"text-align: center;\">Kubernetes frontend</p>\n</td>\n<td style=\"vertical-align:top; padding:0; border:none\"><p class=\"p_Normal\" style=\"text-align: center;\">4</p>\n</td>\n<td style=\"vertical-align:top; padding:0; border:none\"><p class=\"p_Normal\" style=\"text-align: center;\">6</p>\n</td>\n<td style=\"vertical-align:top; padding:0; border:none\"><p class=\"p_Normal\" style=\"text-align: center;\">60</p>\n</td>\n<td style=\"vertical-align:top; padding:0; border:none\"><p class=\"p_Normal\" style=\"text-align: center;\">1</p>\n</td>\n</tr>\n</table>\n</div>\n<ul style=\"list-style-type:disc\">\n<li class=\"p_Normal\">доступ до проксирующего <span style=\"font-weight: bold;\">registry</span> или до частного хранилища образов контейнеров с образами контейнеров Deckhouse;</li></ul>\n<p class=\"p_CodeExample\" style=\"page-break-inside: avoid;\"><span class=\"f_CodeExample\">Начало внимание</span></p>\n<p class=\"p_Normal\">Deckhouse поддерживает работу только с Bearer token схемой авторизации в <span style=\"font-weight: bold;\">registry</span>.</p>\n<p class=\"p_CodeExample\" style=\"page-break-inside: avoid;\"><span class=\"f_CodeExample\">Конец внимание</span></p>\n<ul style=\"list-style-type:disc\">\n<li class=\"p_Normal\">доступ до прокси-сервера для скачивания deb/rpm-пакетов ОС при необходимости;</li><li class=\"p_Normal\">на узле не должно быть установлено пакетов container runtime, например, containerd или docker.</li></ul>\n<p class=\"p_CodeExample\" style=\"page-break-inside: avoid;\"><span class=\"f_CodeExample\">Начало примечание</span></p>\n<p class=\"p_Normal\"><span style=\"font-weight: bold;\">Примечание</span></p>\n<p class=\"p_Normal\">Установка непосредственно с master-узла в настоящий момент не поддерживается. Установщик в виде Docker-образа нельзя запускать на том же узле, на котором планируется развёртывание master-узла, так как на узле не должно быть установлено пакетов container runtime, например, containerd или docker. При отсутствии менеджмент узлов установите docker на любой другой ноде будущего кластера, запустите Docker-образ установщика, установите Deckhouse и удалите Docker-образ установщика c ноды вместе с docker.</p>\n<p class=\"p_CodeExample\" style=\"page-break-inside: avoid;\"><span class=\"f_CodeExample\">Конец примечание</span></p>\n<h2 class=\"p_Heading2\"><a class=\"hmanchor\" id=\"preparation-config-file\"></a><span class=\"f_Heading2\">Шаг 3. Подготовка конфигурационного файла</span></h2>\n<p class=\"p_Normal\">Чтобы установить Deckhouse, подготовьте YAML-файл конфигурации установки. Для получения YAML-файла конфигурации воспользуйтесь сервисом <a class=\"weblink\" href=\"https://deckhouse.ru/gs/\" target=\"_blank\">Быстрый старт</a> на сайте Deckhouse. Сервис сгенерирует актуальный YAML-файл для текущей версий платформы.</p>\n<ol style=\"list-style-type:upper-roman\">\n<li class=\"p_Normal\" value=\"1\">Сгенерируйте YAML-файл сервисом <a class=\"weblink\" href=\"https://deckhouse.ru/gs/\" target=\"_blank\">Быстрый старт</a>, выполнив следующие шаги:</li></ol>\n<ol style=\"list-style-type:decimal\">\n<li class=\"p_Normal\" value=\"1\">Выберите инфраструктуру — Bare Metal.</li><li class=\"p_Normal\" value=\"2\">Ознакомьтесь с информацией об установке.</li><li class=\"p_Normal\" value=\"3\">Укажите шаблон для DNS-имён кластера. В нашем случае — <span style=\"font-weight: bold;\">%s.example.com</span>.</li><li class=\"p_Normal\" value=\"4\">Сохраните <code><b>config.yml</b></code>.</li></ol>\n<ol start=\"2\" style=\"list-style-type:upper-roman\">\n<li class=\"p_Normal\" value=\"2\">Внесите необходимые изменения в <code><b>config.yml</b></code>. Для этого выполните следующие действия:</li></ol>\n<ol style=\"list-style-type:decimal\">\n<li class=\"p_Normal\" value=\"1\">Задайте адресное пространство подов кластера в <span style=\"font-weight: bold;\">podSubnetCIDR</span>.</li><li class=\"p_Normal\" value=\"2\">Задайте адресное пространство Service’ов кластера в <span style=\"font-weight: bold;\">serviceSubnetCIDR</span>.</li><li class=\"p_Normal\" value=\"3\">Задайте нужную версию Kubernetes в <span style=\"font-weight: bold;\">kubernetesVersion</span>.</li><li class=\"p_Normal\" value=\"4\">Проверьте канал обновления в <span style=\"font-weight: bold;\">releaseChannel</span> (EarlyAccess).</li><li class=\"p_Normal\" value=\"5\">Проверьте шаблон доменного имени в <span style=\"font-weight: bold;\">publicDomainTemplate</span> (<span style=\"font-weight: bold;\">%s.example.com</span>).<br/>\nИспользуется для формирования доменов системных приложений в кластере. Например, Grafana для шаблона <span style=\"font-weight: bold;\">%s.example.com</span> будет доступна, как <span style=\"font-weight: bold;\">grafana.example.com</span>.</li><li class=\"p_Normal\" value=\"6\">Проверьте режим работы модуля cni-flannel в <span style=\"font-weight: bold;\">podNetworkMode</span>.<br/>\nРежим работы flannel, допустимые значения <span style=\"font-weight: bold;\">VXLAN</span> (если ваши сервера имеют связность L3) или <span style=\"font-weight: bold;\">HostGW</span> (для L2-сетей).</li><li class=\"p_Normal\" value=\"7\">Укажите локальную сеть, которую будут использовать узлы кластера в <span style=\"font-weight: bold;\">internalNetworkCIDRs</span>.<br/>\nСписок внутренних сетей узлов кластера, например, <code><b>'192.168.1.0/24'</b></code>, который используется для связи компонентов Kubernetes (kube-apiserver, kubelet и т. д.) между собой.</li></ol>\n<p class=\"p_Normal\">Пример файла первичной конфигурации кластера — <code><b>config.yml</b></code>.</p>\n<p class=\"p_Normal\"><span style=\"font-weight: bold;\">Для установки через интернет</span>:</p>\n<p class=\"p_CodeExample\" style=\"page-break-inside: avoid;\"><span class=\"f_CodeExample\">apiVersion: deckhouse.io/v1</span><br/>\n<span class=\"f_CodeExample\">kind: ClusterConfiguration</span><br/>\n<span class=\"f_CodeExample\">clusterType: Static</span><br/>\n<span class=\"f_CodeExample\">podSubnetCIDR: 10.111.0.0/16</span><br/>\n<span class=\"f_CodeExample\">serviceSubnetCIDR: 10.222.0.0/16</span><br/>\n<span class=\"f_CodeExample\">kubernetesVersion: \"Automatic\"</span><br/>\n<span class=\"f_CodeExample\">clusterDomain: \"cluster.local\"</span><br/>\n<span class=\"f_CodeExample\">---</span><br/>\n<span class=\"f_CodeExample\">apiVersion: deckhouse.io/v1</span><br/>\n<span class=\"f_CodeExample\">kind: InitConfiguration</span><br/>\n<span class=\"f_CodeExample\">deckhouse:</span><br/>\n<span class=\"f_CodeExample\">  imagesRepo: registry.deckhouse.ru/deckhouse/ce</span><br/>\n<span class=\"f_CodeExample\">  registryDockerCfg: eyJhdXRocyI6IHsgInJlZ2lzdHJ5LmRlY2tob3VzZS5ydSI6IHt9fX0K</span><br/>\n<span class=\"f_CodeExample\">---</span><br/>\n<span class=\"f_CodeExample\">apiVersion: deckhouse.io/v1alpha1</span><br/>\n<span class=\"f_CodeExample\">kind: ModuleConfig</span><br/>\n<span class=\"f_CodeExample\">metadata:</span><br/>\n<span class=\"f_CodeExample\">  name: deckhouse</span><br/>\n<span class=\"f_CodeExample\">spec:</span><br/>\n<span class=\"f_CodeExample\">  version: 1</span><br/>\n<span class=\"f_CodeExample\">  enabled: true</span><br/>\n<span class=\"f_CodeExample\">  settings:</span><br/>\n<span class=\"f_CodeExample\">    bundle: Default</span><br/>\n<span class=\"f_CodeExample\">    releaseChannel: EarlyAccess</span><br/>\n<span class=\"f_CodeExample\">    logLevel: Info</span><br/>\n<span class=\"f_CodeExample\">---</span><br/>\n<span class=\"f_CodeExample\">apiVersion: deckhouse.io/v1alpha1</span><br/>\n<span class=\"f_CodeExample\">kind: ModuleConfig</span><br/>\n<span class=\"f_CodeExample\">metadata:</span><br/>\n<span class=\"f_CodeExample\">  name: global</span><br/>\n<span class=\"f_CodeExample\">spec:</span><br/>\n<span class=\"f_CodeExample\">  version: 2</span><br/>\n<span class=\"f_CodeExample\">  settings:</span><br/>\n<span class=\"f_CodeExample\">    modules:</span><br/>\n<span class=\"f_CodeExample\">      publicDomainTemplate: \"%s.elewise.local\"</span><br/>\n<span class=\"f_CodeExample\">---</span><br/>\n<span class=\"f_CodeExample\">apiVersion: deckhouse.io/v1alpha1</span><br/>\n<span class=\"f_CodeExample\">kind: ModuleConfig</span><br/>\n<span class=\"f_CodeExample\">metadata:</span><br/>\n<span class=\"f_CodeExample\">  name: user-authn</span><br/>\n<span class=\"f_CodeExample\">spec:</span><br/>\n<span class=\"f_CodeExample\">  version: 2</span><br/>\n<span class=\"f_CodeExample\">  enabled: true</span><br/>\n<span class=\"f_CodeExample\">  settings:</span><br/>\n<span class=\"f_CodeExample\">    controlPlaneConfigurator:</span><br/>\n<span class=\"f_CodeExample\">      dexCAMode: DoNotNeed</span><br/>\n<span class=\"f_CodeExample\">    publishAPI:</span><br/>\n<span class=\"f_CodeExample\">      enabled: true</span><br/>\n<span class=\"f_CodeExample\">      https:</span><br/>\n<span class=\"f_CodeExample\">        mode: Global</span><br/>\n<span class=\"f_CodeExample\">        global:</span><br/>\n<span class=\"f_CodeExample\">          kubeconfigGeneratorMasterCA: \"\"</span><br/>\n<span class=\"f_CodeExample\">---</span><br/>\n<span class=\"f_CodeExample\">apiVersion: deckhouse.io/v1alpha1</span><br/>\n<span class=\"f_CodeExample\">kind: ModuleConfig</span><br/>\n<span class=\"f_CodeExample\">metadata:</span><br/>\n<span class=\"f_CodeExample\">  name: cni-cilium</span><br/>\n<span class=\"f_CodeExample\">spec:</span><br/>\n<span class=\"f_CodeExample\">  version: 1</span><br/>\n<span class=\"f_CodeExample\">  enabled: true</span><br/>\n<span class=\"f_CodeExample\">  settings:</span><br/>\n<span class=\"f_CodeExample\">    tunnelMode: VXLAN</span><br/>\n<span class=\"f_CodeExample\">---</span><br/>\n<span class=\"f_CodeExample\">apiVersion: deckhouse.io/v1</span><br/>\n<span class=\"f_CodeExample\">kind: StaticClusterConfiguration</span><br/>\n<span class=\"f_CodeExample\">internalNetworkCIDRs:</span><br/>\n<span class=\"f_CodeExample\">- 192.168.1.0/24</span></p>\n<p class=\"p_Normal\"><a class=\"dropdown-toggle\" href=\"javascript:HMToggle('toggle','TOGGLE0186A1')\" style=\"font-style: normal; font-weight: normal; color: #000000; background-color: transparent; text-decoration: none;\">Для офлайн-установки без доступа в интернет</a></p>\n<div class=\"dropdown-toggle-body\" id=\"TOGGLE0186A1\" style=\"text-align: left; text-indent: 0; padding: 0 0 0 0; margin: 0 0 0 0;\"><table style=\"border:none; border-spacing:0;\">\n<tr>\n<td style=\"vertical-align:top; padding:0; border:none\"><p class=\"p_CodeExample\" style=\"page-break-inside: avoid;\"><span class=\"f_CodeExample\">Начало внимание</span></p>\n<p class=\"p_Normal\">Для генерации YAML-файла сервисом <a class=\"weblink\" href=\"https://deckhouse.ru/gs/\" target=\"_blank\">Быстрый старт</a> выберите инфраструктуру — <span style=\"font-weight: bold;\">Закрытое окружение</span>.</p>\n<p class=\"p_CodeExample\" style=\"page-break-inside: avoid;\"><span class=\"f_CodeExample\">Конец внимание</span></p>\n<p class=\"p_Normal\">Установите следующие параметры в ресурсе <span style=\"font-weight: bold;\">InitConfiguration</span>:</p>\n<ul style=\"list-style-type:disc\">\n<li class=\"p_Normal\"><span style=\"font-weight: bold;\">devBranch</span>: если в изолированном приватном репозитории нет образов, содержащих информацию о каналах обновлений, используйте точный тег образа Deckhouse, чтобы установить Deckhouse Platform. Например, если вы хотите установить релиз v1.46.3, используйте образ <code><b>registry.example.com/images/deckhouse/install:v1.46.3</b></code>. Также укажите <code><b>devBranch: v1.46.3</b></code> вместо <code><b>releaseChannel: XXX</b></code>;</li></ul>\n<ul style=\"list-style-type:disc\">\n<li class=\"p_Normal\"><span style=\"font-weight: bold;\">imagesRepo</span>: <code><b>&lt;PROXY_REGISTRY&gt;/&lt;DECKHOUSE_REPO_PATH&gt;/&lt;DECKHOUSE_REVISION&gt;</b></code> — адрес образа Deckhouse в приватном репозитории с учётом используемой редакции. В рамках статьи образы были загружены в <code><b>registry.example.com/images/deckhouse</b></code>. Подробнее читайте в статье <a class=\"topiclink\" href=\"downloading-images-deckhouse.html\">«Загрузка образов Deckhouse»</a>;</li></ul>\n<ul style=\"list-style-type:disc\">\n<li class=\"p_Normal\"><span style=\"font-weight: bold;\">registryDockerCfg</span>: <code><b>&lt;BASE64&gt;</b></code> — права доступа к приватному репозиторию, зашифрованные в Base64. Примеры заполнения <span style=\"font-weight: bold;\">registryDockerCfg</span> читайте в официальной документации <a class=\"weblink\" href=\"https://deckhouse.ru/documentation/v1/deckhouse-faq.html#%D0%BF%D0%BE%D0%B4%D0%B3%D0%BE%D1%82%D0%BE%D0%B2%D0%BA%D0%B0-%D0%BA%D0%BE%D0%BD%D1%84%D0%B8%D0%B3%D1%83%D1%80%D0%B0%D1%86%D0%B8%D0%B8\" target=\"_blank\">Deckhouse Kubernetes Platform</a>. В рамках статьи разрешён анонимный доступ к образам Deckhouse в стороннем <span style=\"font-weight: bold;\">registry</span>. Сформируйте <span style=\"font-weight: bold;\">registryDockerCfg</span>, выполнив следующую команду:</li></ul>\n<p class=\"p_CodeExample\" style=\"page-break-inside: avoid;\"><span class=\"f_CodeExample\">echo -n \"{\\\"auths\\\": { \\\"registry.example.com:443/images/deckhouse\\\": {}}}\" | base64</span></p>\n<ul style=\"list-style-type:disc\">\n<li class=\"p_Normal\"><span style=\"font-weight: bold;\">registryScheme</span>: укажите, по какому протоколу (HTTP, HTTPS) работает приватный репозиторий;</li></ul>\n<ul style=\"list-style-type:disc\">\n<li class=\"p_Normal\"><span style=\"font-weight: bold;\">registryCA</span>: корневой SSL-сертификат, которым можно проверить SSL-сертификат приватного реестра, например, если хранилище использует самоподписанный сертификат. Если вы используете не самоподписанный сертификат или хранилище работает по протоколу HTTP, удалите этот параметр.</li></ul>\n<p class=\"p_Normal\">Пример файла первичной конфигурации кластера — <code><b>config.yml</b></code>:</p>\n<p class=\"p_CodeExample\" style=\"white-space: normal; page-break-inside: avoid;\"><span class=\"f_CodeExample\">apiVersion: deckhouse.io/v1</span><br/>\n<span class=\"f_CodeExample\">kind: ClusterConfiguration</span><br/>\n<span class=\"f_CodeExample\">clusterType: Static</span><br/>\n<span class=\"f_CodeExample\">podSubnetCIDR: 10.111.0.0/16</span><br/>\n<span class=\"f_CodeExample\">serviceSubnetCIDR: 10.222.0.0/16</span><br/>\n<span class=\"f_CodeExample\">kubernetesVersion: \"</span><span class=\"f_CodeExample\">Automatic</span><span class=\"f_CodeExample\">\"</span><br/>\n<span class=\"f_CodeExample\">clusterDomain: \"cluster.local\"</span><br/>\n<span class=\"f_CodeExample\">---</span><br/>\n<span class=\"f_CodeExample\">apiVersion: deckhouse.io/v1</span><br/>\n<span class=\"f_CodeExample\">kind: InitConfiguration</span><br/>\n<span class=\"f_CodeExample\">deckhouse:</span><br/>\n<span class=\"f_CodeExample\">  devBranch: v1.46.3</span><br/>\n<span class=\"f_CodeExample\">  configOverrides:</span><br/>\n<span class=\"f_CodeExample\">    global:</span><br/>\n<span class=\"f_CodeExample\">      modules:</span><br/>\n<span class=\"f_CodeExample\">        publicDomainTemplate: \"%s.example.com\"</span><br/>\n<span class=\"f_CodeExample\">    cniFlannelEnabled: </span><span class=\"f_CodeExample\" style=\"font-weight: bold;\">true</span><br/>\n<span class=\"f_CodeExample\">    cniFlannel:</span><br/>\n<span class=\"f_CodeExample\">      podNetworkMode: VXLAN</span><br/>\n<span class=\"f_CodeExample\">  imagesRepo: registry.example.com:443/images/deckhouse</span><br/>\n<span class=\"f_CodeExample\">  registryDockerCfg: eyJhdXRocyI6IHsgInJlZ2lzdHJ5LmV4YW1wbGUuY29tOjQ0My9pbWFnZXMvZGVja2hvdXNlIjoge319fQ==</span><br/>\n<span class=\"f_CodeExample\">  registryScheme: HTTPS</span><br/>\n<span class=\"f_CodeExample\">  registryCA: |</span><br/>\n<span class=\"f_CodeExample\">    -----BEGIN CERTIFICATE-----</span><br/>\n<span class=\"f_CodeExample\">    MIIFBzCCGu+gAwIBAgIUBZ37mm02QGGcmd5pZvWwnpCfQUowDQYGKoZIhvcNAQEL</span><br/>\n<span class=\"f_CodeExample\">    BQAwHjEcMBoGA1UEAwwTaW1hZ2VzLnByb2FjdG9yLnBybzAeFw0yMjA5MjkxNDUw</span><br/>\n<span class=\"f_CodeExample\">    ...</span><br/>\n<span class=\"f_CodeExample\">    9UpckrwxPhctmln5/Awd/2gcaRAxI3qBL7SyDFT0YpnGcAiGPY4Z67HhZ7h6y+2F</span><br/>\n<span class=\"f_CodeExample\">    fQDSXli0r61/Fenkh5OLMihLYTm+5gjZlG1LCXpaGIpjAf16Q+3/pIqapQ==</span><br/>\n<span class=\"f_CodeExample\">    -----END CERTIFICATE-----</span><br/>\n<span class=\"f_CodeExample\">---</span><br/>\n<span class=\"f_CodeExample\">apiVersion: deckhouse.io/v1</span><br/>\n<span class=\"f_CodeExample\">kind: StaticClusterConfiguration</span><br/>\n<span class=\"f_CodeExample\">internalNetworkCIDRs:</span><br/>\n<span class=\"f_CodeExample\">  - 192.168.1.0/24</span></p>\n</td>\n</tr>\n</table>\n</div>\n<h2 class=\"p_Heading2\"><a class=\"hmanchor\" id=\"installation\"></a><span class=\"f_Heading2\">Шаг 4. Установка Kubernetes-кластера на базе Deckhouse</span></h2>\n<p class=\"p_Normal\">Установка Deckhouse Platform Community Edition сводится к установке кластера, который состоит из единственного master-узла. Инсталлятор Deckhouse доступен в виде образа контейнера, в который необходимо передать конфигурационные файлы и SSH-ключи доступа на master-узел. Далее подразумевается, что используется SSH-ключ <code><b>~/.ssh/id_rsa</b></code>. В основе инсталлятора лежит утилита <a class=\"weblink\" href=\"https://github.com/deckhouse/deckhouse/tree/main/dhctl/\" target=\"_blank\">dhctl</a>.</p>\n<ol style=\"list-style-type:upper-roman\">\n<li class=\"p_Normal\" value=\"1\">Запустите установщик.</li></ol>\n<p class=\"p_CodeExample\" style=\"page-break-inside: avoid;\"><span class=\"f_CodeExample\">Начало примечание</span></p>\n<p class=\"p_Normal\"><span style=\"font-weight: bold;\">Примечание</span></p>\n<p class=\"p_Normal\">Установка непосредственно с master-узла в настоящий момент не поддерживается. Установщик в виде Docker-образа нельзя запускать на том же узле, на котором планируется развёртывание master-узла, так как на узле не должно быть установлено пакетов container runtime, например, containerd или docker.</p>\n<p class=\"p_CodeExample\" style=\"page-break-inside: avoid;\"><span class=\"f_CodeExample\">Конец примечание</span></p>\n<p class=\"p_Normal\">Установщик запускается на персональном компьютере, подготовленном на этапе <a class=\"topiclink\" href=\"fail-safe-kubernetes-cluster.html#cluster-architecture\">архитектура кластера Kubernetes</a>. На ПК перейдите в директорию с файлом конфигурации <code><b>config.yml</b></code>, подготовленным на этапе <a class=\"topiclink\" href=\"fail-safe-kubernetes-cluster.html#preparation-config-file\">подготовка конфигурационного файла</a>.</p>\n<p class=\"p_Normal\">Для запуска установщика через интернет выполните команду:</p>\n<p class=\"p_CodeExample\" style=\"page-break-inside: avoid;\"><span class=\"f_CodeExample\">sudo docker run --pull=always -it -v \"$PWD/config.yml:/config.yml\" -v \"$HOME/.ssh/:/tmp/.ssh/\" registry.deckhouse.io/deckhouse/ce/install:stable bash</span></p>\n<p class=\"p_Normal\"><a class=\"dropdown-toggle\" href=\"javascript:HMToggle('toggle','TOGGLE0186A2')\" style=\"font-style: normal; font-weight: normal; color: #000000; background-color: transparent; text-decoration: none;\">Для офлайн-установки без доступа в интернет</a></p>\n<div class=\"dropdown-toggle-body\" id=\"TOGGLE0186A2\" style=\"text-align: left; text-indent: 0; padding: 0 0 0 0; margin: 0 0 0 0;\"><table style=\"border:none; border-spacing:0;\">\n<tr>\n<td style=\"vertical-align:top; padding:0; border:none\"><p class=\"p_Normal\"> <br/>\nВыполните команду:</p>\n<p class=\"p_CodeExample\" style=\"page-break-inside: avoid;\"><span class=\"f_CodeExample\">sudo docker run --pull=always -it -v \"$PWD/config.yml:/config.yml\" -v \"$HOME/.ssh/:/tmp/.ssh/\" example.com:443/images/deckhouse/install:v1.46.3 bash</span></p>\n<p class=\"p_Normal\"> <br/>\nГде:</p>\n<p class=\"p_Normal\"><code><b>example.com:443/images/deckhouse/install:v1.46.3</b></code> — версия устанавливаемого релиза.</p>\n</td>\n</tr>\n</table>\n</div>\n<ol start=\"2\" style=\"list-style-type:upper-roman\">\n<li class=\"p_Normal\" value=\"2\">Установите Deckhouse. Для этого внутри контейнера установщика выполните команду:</li></ol>\n<p class=\"p_CodeExample\" style=\"page-break-inside: avoid;\"><span class=\"f_CodeExample\">dhctl bootstrap --ssh-user=&lt;username&gt; --ssh-host=&lt;master_ip&gt; --ssh-agent-</span><span class=\"f_CodeExample\" style=\"font-weight: bold;\">private</span><span class=\"f_CodeExample\">-keys=/tmp/.ssh/id_rsa \\</span><br/>\n<span class=\"f_CodeExample\">--config=/config.yml \\</span><br/>\n<span class=\"f_CodeExample\">--ask-become-pass</span></p>\n<p class=\"p_Normal\">где:</p>\n<ul style=\"list-style-type:disc\">\n<li class=\"p_CodeExample\" style=\"white-space: normal; page-break-inside: auto;\"><code><b>&lt;username&gt;</b></code> — в параметре <code><b>--ssh-user</b></code> укажите имя пользователя, от которого генерировался SSH-ключ для установки;</li><li class=\"p_CodeExample\" style=\"white-space: normal; page-break-inside: auto;\"><code><b>&lt;master_ip&gt;</b></code> — IP адрес master-узла подготовленного на этапе <a class=\"topiclink\" href=\"fail-safe-kubernetes-cluster.html#cluster-architecture\">архитектура кластера Kubernetes</a>.</li></ul>\n<p class=\"p_Normal\">Процесс установки может занять 15-30 минут при хорошем соединении.</p>\n<h2 class=\"p_Heading2\"><a class=\"hmanchor\" id=\"addition-frontend-nodes\"></a><span class=\"f_Heading2\">Шаг 5. Добавление frontend-узлов</span></h2>\n<p class=\"p_Normal\">Перед добавлением frontend-нод предварительно создайте новый custom resource <a class=\"weblink\" href=\"https://deckhouse.ru/documentation/latest/modules/040-node-manager/cr.html#nodegroup\" target=\"_blank\">NodeGroup</a> с именем <code><b>frontend</b></code>. Параметр <code><b>nodeType</b></code> в custom resource NodeGroup задайте как <code><b>Static</b></code>.</p>\n<ol style=\"list-style-type:upper-roman\">\n<li class=\"p_Normal\" value=\"1\">Создайте на ноде master 1 файл <code><b>frontend.yaml</b></code> с описанием статичной NodeGroup с наименованием <code><b>frontend</b></code>:</li></ol>\n<p class=\"p_CodeExample\" style=\"page-break-inside: avoid;\"><span class=\"f_CodeExample\">apiVersion: deckhouse.io/v1</span><br/>\n<span class=\"f_CodeExample\">kind: NodeGroup</span><br/>\n<span class=\"f_CodeExample\">metadata:</span><br/>\n<span class=\"f_CodeExample\">  name: frontend</span><br/>\n<span class=\"f_CodeExample\">spec:</span><br/>\n<span class=\"f_CodeExample\">  nodeTemplate:</span><br/>\n<span class=\"f_CodeExample\">    labels:</span><br/>\n<span class=\"f_CodeExample\">      node-role.deckhouse.io/frontend: \"\"</span><br/>\n<span class=\"f_CodeExample\">    taints:</span><br/>\n<span class=\"f_CodeExample\">      - effect: NoExecute</span><br/>\n<span class=\"f_CodeExample\">        key: dedicated.deckhouse.io</span><br/>\n<span class=\"f_CodeExample\">        value: frontend</span><br/>\n<span class=\"f_CodeExample\">  nodeType: Static</span></p>\n<ol start=\"2\" style=\"list-style-type:upper-roman\">\n<li class=\"p_Normal\" value=\"2\">Примените файл <code><b>frontend.yaml</b></code>, выполнив команду:</li></ol>\n<p class=\"p_CodeExample\" style=\"page-break-inside: avoid;\"><span class=\"f_CodeExample\">kubectl create -f frontend.yaml</span></p>\n<ol start=\"3\" style=\"list-style-type:upper-roman\">\n<li class=\"p_Normal\" value=\"3\">Для добавления frontend-нод выполните следующие действия:</li></ol>\n<ol style=\"list-style-type:decimal\">\n<li class=\"p_Normal\" value=\"1\">Получите код скрипта в кодировке Base64 для добавления и настройки нового узла frontend, выполнив команду на ноде master 1:</li></ol>\n<p class=\"p_CodeExample\" style=\"page-break-inside: avoid;\"><span class=\"f_CodeExample\">kubectl -n d8-cloud-instance-manager get secret manual-bootstrap-</span><span class=\"f_CodeExample\" style=\"font-weight: bold;\">for</span><span class=\"f_CodeExample\">-frontend -o json | jq '.data.\"bootstrap.sh\"' -r</span></p>\n<ol start=\"2\" style=\"list-style-type:decimal\">\n<li class=\"p_Normal\" value=\"2\">Зайдите на узел, который хотите добавить по SSH (в данном случае frontend 1) и вставьте полученную на первом шаге Base64‑строку:</li></ol>\n<p class=\"p_CodeExample\" style=\"page-break-inside: avoid;\"><span class=\"f_CodeExample\">echo &lt;Base64-КОД-СКРИПТА&gt; | base64 -d | bash</span></p>\n<p class=\"p_Normal\">Дождитесь окончания выполнения скрипта. Нода добавлена.</p>\n<p class=\"p_Normal\">Чтобы добавить новые frontend-ноды, выполните действия из пункта 3.</p>\n<h2 class=\"p_Heading2\"><a class=\"hmanchor\" id=\"addition-system-nodes\"></a><span class=\"f_Heading2\">Шаг 6. Добавление system-узлов</span></h2>\n<p class=\"p_Normal\">Перед добавлением system-нод предварительно создайте новый custom resource <a class=\"weblink\" href=\"https://deckhouse.ru/documentation/latest/modules/040-node-manager/cr.html#nodegroup\" target=\"_blank\">NodeGroup</a> с именем <code><b>system</b></code>. Параметр <code><b>nodeType</b></code> в custom resource NodeGroup задать как <code><b>Static</b></code>.</p>\n<ol style=\"list-style-type:upper-roman\">\n<li class=\"p_Normal\" value=\"1\">Создайте на ноде master 1 файл <code><b>system.yaml</b></code> с описанием статичной NodeGroup с наименованием <code><b>system</b></code>:</li></ol>\n<p class=\"p_CodeExample\" style=\"page-break-inside: avoid;\"><span class=\"f_CodeExample\">apiVersion: deckhouse.io/v1</span><br/>\n<span class=\"f_CodeExample\">kind: NodeGroup</span><br/>\n<span class=\"f_CodeExample\">metadata:</span><br/>\n<span class=\"f_CodeExample\">  name: system</span><br/>\n<span class=\"f_CodeExample\">spec:</span><br/>\n<span class=\"f_CodeExample\">  nodeTemplate:</span><br/>\n<span class=\"f_CodeExample\">    labels:</span><br/>\n<span class=\"f_CodeExample\">      node-role.deckhouse.io/system: \"\"</span><br/>\n<span class=\"f_CodeExample\">    taints:</span><br/>\n<span class=\"f_CodeExample\">      - effect: NoExecute</span><br/>\n<span class=\"f_CodeExample\">        key: dedicated.deckhouse.io</span><br/>\n<span class=\"f_CodeExample\">        value: system</span><br/>\n<span class=\"f_CodeExample\">  nodeType: Static</span></p>\n<ol start=\"2\" style=\"list-style-type:upper-roman\">\n<li class=\"p_Normal\" value=\"2\">Примените файл <code><b>system.yaml</b></code>, выполнив команду:</li></ol>\n<p class=\"p_CodeExample\" style=\"page-break-inside: avoid;\"><span class=\"f_CodeExample\">kubectl create -f system.yaml</span></p>\n<ol start=\"3\" style=\"list-style-type:upper-roman\">\n<li class=\"p_Normal\" value=\"3\">Для добавления system-нод выполните следующие действия:</li></ol>\n<ol style=\"list-style-type:decimal\">\n<li class=\"p_Normal\" value=\"1\">Получите код скрипта в кодировке Base64 для добавления и настройки нового узла system, выполнив команду на ноде master 1:</li></ol>\n<p class=\"p_CodeExample\" style=\"page-break-inside: avoid;\"><span class=\"f_CodeExample\">kubectl -n d8-cloud-instance-manager get secret manual-bootstrap-</span><span class=\"f_CodeExample\" style=\"font-weight: bold;\">for</span><span class=\"f_CodeExample\">-system -o json | jq '.data.\"bootstrap.sh\"' -r</span></p>\n<ol start=\"2\" style=\"list-style-type:decimal\">\n<li class=\"p_Normal\" value=\"2\">Зайдите на узел, который хотите добавить по SSH (в данном случае system 1) и вставьте полученную на первом шаге Base64‑строку:</li></ol>\n<p class=\"p_CodeExample\" style=\"page-break-inside: avoid;\"><span class=\"f_CodeExample\">echo &lt;Base64-КОД-СКРИПТА&gt; | base64 -d | bash</span></p>\n<p class=\"p_Normal\">Дождитесь окончания выполнения скрипта. Нода добавлена.</p>\n<p class=\"p_Normal\">Чтобы добавить новые system-ноды, выполните действия из пункта 3.</p>\n<h2 class=\"p_Heading2\"><a class=\"hmanchor\" id=\"addition-worke-nodes\"></a><span class=\"f_Heading2\">Шаг 7. Добавление worker-узлов</span></h2>\n<p class=\"p_Normal\">Перед добавлением worker-нод предварительно создайте новый custom resource <a class=\"weblink\" href=\"https://deckhouse.ru/documentation/latest/modules/040-node-manager/cr.html#nodegroup\" target=\"_blank\">NodeGroup</a> с именем <code><b>worker</b></code>. Параметр <code><b>nodeType</b></code> в custom resource NodeGroup задать как <code><b>Static</b></code>.</p>\n<ol style=\"list-style-type:upper-roman\">\n<li class=\"p_Normal\" value=\"1\">Создайте на ноде master 1 файл <code><b>worker.yaml</b></code> с описанием статичной NodeGroup с наименованием <code><b>worker</b></code>:</li></ol>\n<p class=\"p_CodeExample\" style=\"page-break-inside: avoid;\"><span class=\"f_CodeExample\">apiVersion: deckhouse.io/v1</span><br/>\n<span class=\"f_CodeExample\">kind: NodeGroup</span><br/>\n<span class=\"f_CodeExample\">metadata:</span><br/>\n<span class=\"f_CodeExample\">  name: worker</span><br/>\n<span class=\"f_CodeExample\">spec:</span><br/>\n<span class=\"f_CodeExample\">  nodeType: Static</span><br/>\n<span class=\"f_CodeExample\">  kubelet:</span><br/>\n<span class=\"f_CodeExample\">    maxPods: 200</span></p>\n<ol start=\"2\" style=\"list-style-type:upper-roman\">\n<li class=\"p_Normal\" value=\"2\">Примените файл <code><b>worker.yaml</b></code>, выполнив команду:</li></ol>\n<p class=\"p_CodeExample\" style=\"page-break-inside: avoid;\"><span class=\"f_CodeExample\">kubectl create -f worker.yaml</span></p>\n<ol start=\"3\" style=\"list-style-type:upper-roman\">\n<li class=\"p_Normal\" value=\"3\">Для добавления worker-нод выполните следующие действия:</li></ol>\n<ol style=\"list-style-type:decimal\">\n<li class=\"p_Normal\" value=\"1\">Получите код скрипта в кодировке Base64 для добавления и настройки нового узла worker, выполнив команду на ноде master 1:</li></ol>\n<p class=\"p_CodeExample\" style=\"page-break-inside: avoid;\"><span class=\"f_CodeExample\">kubectl -n d8-cloud-instance-manager get secret manual-bootstrap-</span><span class=\"f_CodeExample\" style=\"font-weight: bold;\">for</span><span class=\"f_CodeExample\">-worker -o json | jq '.data.\"bootstrap.sh\"' -r</span></p>\n<ol start=\"2\" style=\"list-style-type:decimal\">\n<li class=\"p_Normal\" value=\"2\">Зайдите на узел, который хотите добавить по SSH (в данном случае worker 1) и вставьте полученную на первом шаге Base64-строку:</li></ol>\n<p class=\"p_CodeExample\" style=\"page-break-inside: avoid;\"><span class=\"f_CodeExample\">echo &lt;Base64-КОД-СКРИПТА&gt; | base64 -d | bash</span></p>\n<p class=\"p_Normal\">Дождитесь окончания выполнения скрипта. Нода добавлена.</p>\n<p class=\"p_Normal\">Чтобы добавить новые worker-ноды, выполните действия из пункта 3.</p>\n<h2 class=\"p_Heading2\"><a class=\"hmanchor\" id=\"addition-master-nodes\"></a><span class=\"f_Heading2\">Шаг 8. Добавление master-узлов</span></h2>\n<ol style=\"list-style-type:upper-roman\">\n<li class=\"p_Normal\" value=\"1\">Получите код скрипта в кодировке Base64 для добавления и настройки нового узла master, выполнив команду на ноде master 1:</li></ol>\n<p class=\"p_CodeExample\" style=\"page-break-inside: avoid;\"><span class=\"f_CodeExample\">kubectl -n d8-cloud-instance-manager get secret manual-bootstrap-</span><span class=\"f_CodeExample\" style=\"font-weight: bold;\">for</span><span class=\"f_CodeExample\">-master -o json | jq '.data.\"bootstrap.sh\"' -r</span></p>\n<ol start=\"2\" style=\"list-style-type:upper-roman\">\n<li class=\"p_Normal\" value=\"2\">Зайдите на узел, который хотите добавить по SSH (в данном случае master 2) и вставьте полученную на первом шаге Base64-строку:</li></ol>\n<p class=\"p_CodeExample\" style=\"page-break-inside: avoid;\"><span class=\"f_CodeExample\">echo &lt;Base64-КОД-СКРИПТА&gt; | base64 -d | bash</span></p>\n<p class=\"p_Normal\">Дождитесь окончания выполнения скрипта. Нода добавлена.</p>\n<p class=\"p_Normal\">Чтобы добавить новые master-ноды, выполните действия из шага 8.</p>\n<h2 class=\"p_Heading2\"><a class=\"hmanchor\" id=\"addition-local-path-provisioner\"></a><span class=\"f_Heading2\">Шаг 9. Добавление Local Path Provisioner</span></h2>\n<p class=\"p_Normal\">По умолчанию storageclass отсутствует в Deckhouse. Создайте custom resource <span style=\"font-weight: bold;\">LocalPathProvisioner</span>, позволяющий пользователям Kubernetes использовать локальное хранилище на узлах. Для этого выполните следующие действия:</p>\n<ol style=\"list-style-type:upper-roman\">\n<li class=\"p_Normal\" value=\"1\">Создайте на master-узле файл <code><b>local-path-provisioner.yaml</b></code>, содержащий конфигурацию для <span style=\"font-weight: bold;\">LocalPathProvisioner</span>.</li></ol>\n<ol start=\"2\" style=\"list-style-type:upper-roman\">\n<li class=\"p_Normal\" value=\"2\">Установите нужную Reclaim policy (по умолчанию устанавливается Retain). В рамках статьи для параметра <code><b>reclaimPolicy</b></code> установлено <code><b>\"Delete\"</b></code> (PV после удаления PVC удаляются).</li></ol>\n<p class=\"p_Normal\">Пример файла <code><b>local-path-provisioner.yaml</b></code>:</p>\n<p class=\"p_CodeExample\" style=\"page-break-inside: avoid;\"><span class=\"f_CodeExample\">apiVersion: deckhouse.io/v1alpha1</span><br/>\n<span class=\"f_CodeExample\">kind: LocalPathProvisioner</span><br/>\n<span class=\"f_CodeExample\">metadata:</span><br/>\n<span class=\"f_CodeExample\">  name: localpath-deckhouse-system</span><br/>\n<span class=\"f_CodeExample\">spec:</span><br/>\n<span class=\"f_CodeExample\">  nodeGroups:</span><br/>\n<span class=\"f_CodeExample\">  - system</span><br/>\n<span class=\"f_CodeExample\">  - worker</span><br/>\n<span class=\"f_CodeExample\">  path: \"/opt/local-path-provisioner\"</span><br/>\n<span class=\"f_CodeExample\">  reclaimPolicy: \"Delete\"</span></p>\n<ol start=\"3\" style=\"list-style-type:upper-roman\">\n<li class=\"p_Normal\" value=\"3\">Примените файл <code><b>local-path-provisioner.yaml</b></code> в Kubernetes. Для этого выполните команду:</li></ol>\n<p class=\"p_CodeExample\" style=\"page-break-inside: avoid;\"><span class=\"f_CodeExample\">kubectl apply -f local-path-provisioner.yaml</span></p>\n<ol start=\"4\" style=\"list-style-type:upper-roman\">\n<li class=\"p_Normal\" value=\"4\">Установите созданный <span style=\"font-weight: bold;\">LocalPathProvisioner</span>, как storageclass по умолчанию (default-class), выполнив одну из команд:</li></ol>\n<p class=\"p_CodeExample\" style=\"page-break-inside: avoid;\"><span class=\"f_CodeExample\">kubectl patch storageclass localpath-deckhouse-system -p '{\"metadata\": {\"annotations\":{\"storageclass.kubernetes.io/is-default-class\":\"true\"}}}'</span></p>\n<p class=\"p_Normal\">или</p>\n<p class=\"p_CodeExample\" style=\"page-break-inside: avoid;\"><span class=\"f_CodeExample\">sudo -i d8 k patch mc global --type merge -p “{\"spec\": {\"settings\":{\"defaultClusterStorageClass\":\"localpath-deckhouse-system\"}}}”</span></p>\n<p class=\"p_Normal\"><span style=\"font-weight: bold;\">LocalPathProvisioner</span> с наименованием localpath-deckhouse-system создан и готов предоставлять локальные хранилища на узлах c NodeGroup system и worker.</p>\n<h2 class=\"p_Heading2\"><a class=\"hmanchor\" id=\"installation-helm\"></a><span class=\"f_Heading2\">Шаг 10. Установка HELM</span></h2>\n<ol style=\"list-style-type:upper-roman\">\n<li class=\"p_Normal\" value=\"1\">Перейдите на страницу релизов <a class=\"weblink\" href=\"https://github.com/helm/helm/releases\" target=\"_blank\">Helm</a> и скачайте архив <code><b>helm-vX.Y.Z-linux-amd64.tar.gz</b></code> нужно версии.</li></ol>\n<p class=\"p_Normal\">Для установки через интернет:</p>\n<p class=\"p_CodeExample\" style=\"page-break-inside: avoid;\"><span class=\"f_CodeExample\">wget https://get.helm.sh/helm-vX.Y.Z-linux-amd64.tar.gz</span></p>\n<p class=\"p_Normal\"><a class=\"dropdown-toggle\" href=\"javascript:HMToggle('toggle','TOGGLE0186A3')\" style=\"font-style: normal; font-weight: normal; color: #000000; background-color: transparent; text-decoration: none;\">Для офлайн-установки без доступа в интернет</a></p>\n<div class=\"dropdown-toggle-body\" id=\"TOGGLE0186A3\" style=\"text-align: left; text-indent: 0; padding: 0 0 0 0; margin: 0 0 0 0;\"><table style=\"border:none; border-spacing:0;\">\n<tr>\n<td style=\"vertical-align:top; padding:0; border:none\"><ol style=\"list-style-type:decimal\">\n<li class=\"p_Normal\" value=\"1\">На компьютере с доступом в интернет перейдите на страницу релизов <a class=\"weblink\" href=\"https://github.com/helm/helm/releases\" target=\"_blank\">Helm</a> и скачайте архив <code><b>helm-vX.Y.Z-linux-amd64.tar.gz</b></code> нужной версии, выполнив команду:</li></ol>\n<p class=\"p_CodeExample\" style=\"page-break-inside: avoid;\"><span class=\"f_CodeExample\">wget https://get.helm.sh/helm-vX.Y.Z-linux-amd64.tar.gz</span></p>\n<ol start=\"2\" style=\"list-style-type:decimal\">\n<li class=\"p_Normal\" value=\"2\">Скопируйте полученный архив на master-узел.</li></ol>\n</td>\n</tr>\n</table>\n</div>\n<ol start=\"2\" style=\"list-style-type:upper-roman\">\n<li class=\"p_Normal\" value=\"2\">Распакуйте архив и переместите бинарный файл helm:</li></ol>\n<p class=\"p_CodeExample\" style=\"page-break-inside: avoid;\"><span class=\"f_CodeExample\">tar -zxvf helm-vX.Y.Z-linux-amd64.tar.gz</span><br/>\n<span class=\"f_CodeExample\">mv linux-amd64/helm /usr/local/bin/helm</span></p>\n<h2 class=\"p_Heading2\"><a class=\"hmanchor\" id=\"addition-balancer\"></a><span class=\"f_Heading2\">Шаг 11. Добавление балансировщика OpenELB-VIP</span></h2>\n<p class=\"p_Normal\">Для правильной работы Ingress-контроллера требуется прямой выход в интернет с белым IP-адресом на Ingress-узле кластера с использованием NodePort, или можно установить балансировщик <a class=\"weblink\" href=\"https://openelb.io/docs/getting-started/installation/\" target=\"_blank\">OpenELB</a>, который будет заниматься балансировкой трафика так, как это организовано у любого облачного провайдера. Этот балансировщик использует Speaker для поддержки IP-адреса службы.</p>\n<p class=\"p_Normal\">Разверните OpenELB в режиме VIP Mode. Для этого:</p>\n<ol style=\"list-style-type:upper-roman\">\n<li class=\"p_Normal\" value=\"1\">Получите конфигурационный файл <code><b>values-openelb.yaml</b></code>.</li></ol>\n<p class=\"p_Normal\">Для установки через интернет:</p>\n<p class=\"p_CodeExample\" style=\"page-break-inside: avoid;\"><span class=\"f_CodeExample\">helm repo add elma365 https:</span><span class=\"f_CodeExample\">//charts.elma365.tech</span><br/>\n<span class=\"f_CodeExample\">helm repo update</span><br/>\n<span class=\"f_CodeExample\">helm show values elma365/openelb &gt; values-openelb.yaml</span></p>\n<p class=\"p_Normal\"><a class=\"dropdown-toggle\" href=\"javascript:HMToggle('toggle','TOGGLE0186A4')\" style=\"font-style: normal; font-weight: normal; color: #000000; background-color: transparent; text-decoration: none;\">Получение конфигурационного файла для установки в закрытом контуре без доступа в интернет</a></p>\n<div class=\"dropdown-toggle-body\" id=\"TOGGLE0186A4\" style=\"text-align: left; text-indent: 0; padding: 0 0 0 0; margin: 0 0 0 0;\"><table style=\"border:none; border-spacing:0;\">\n<tr>\n<td style=\"vertical-align:top; padding:0; border:none\"><ol style=\"list-style-type:decimal\">\n<li class=\"p_Normal\" value=\"1\">На компьютере с доступом в интернет скачайте архив актуальной версии (latest) чарта OpenELB из репозитория elma365, выполнив следующую команду:</li></ol>\n<p class=\"p_CodeExample\" style=\"page-break-inside: avoid;\"><span class=\"f_CodeExample\">helm repo add elma365 https:</span><span class=\"f_CodeExample\">//charts.elma365.tech</span><br/>\n<span class=\"f_CodeExample\">helm repo update</span><br/>\n<span class=\"f_CodeExample\">helm pull elma365/openelb</span></p>\n<ol start=\"2\" style=\"list-style-type:decimal\">\n<li class=\"p_Normal\" value=\"2\">Полученный архив чарта <code><b>openelb-X.Y.Z.tgz</b></code> скопируйте на сервер, где будет производиться установка.</li></ol>\n<ol start=\"3\" style=\"list-style-type:decimal\">\n<li class=\"p_Normal\" value=\"3\">Распакуйте чарт <code><b>openelb-X.Y.Z.tgz</b></code> на сервере, где будет производиться установка и скопируйте конфигурационный файл по умолчанию <code><b>values.yaml</b></code> в <code><b>values-openelb.yaml</b></code>.</li></ol>\n<p class=\"p_CodeExample\" style=\"page-break-inside: avoid;\"><span class=\"f_CodeExample\">tar -xf openelb-X.Y.Z.tgz</span><br/>\n<span class=\"f_CodeExample\">cp openelb/values.yaml values-openelb.yaml</span></p>\n</td>\n</tr>\n</table>\n</div>\n<ol start=\"2\" style=\"list-style-type:upper-roman\">\n<li class=\"p_Normal\" value=\"2\">Измените конфигурационный файл <code><b>values-openelb.yaml</b></code>.</li></ol>\n<p class=\"p_Normal\">Запланируйте размещение подов openelb-controller на frontend-нодах кластера. Для этого измените секции <span style=\"font-weight: bold;\">tolerations</span> и <span style=\"font-weight: bold;\">nodeSelector</span>:</p>\n<p class=\"p_CodeExample\" style=\"white-space: normal; page-break-inside: avoid;\"><span class=\"f_CodeExample\">## Настройки openelb</span><br/>\n<span class=\"f_CodeExample\">openelb:</span><br/>\n<span class=\"f_CodeExample\">  speaker:</span><br/>\n<span class=\"f_CodeExample\">    enable: true</span><br/>\n<span class=\"f_CodeExample\">    vip: true</span><br/>\n<span class=\"f_CodeExample\">    apiHosts: \":50051\"</span><br/>\n<span class=\"f_CodeExample\">    tolerations:</span><br/>\n<span class=\"f_CodeExample\">      - key: CriticalAddonsOnly</span><br/>\n<span class=\"f_CodeExample\">        operator: Exists</span><br/>\n<span class=\"f_CodeExample\">      - effect: NoExecute</span><br/>\n<span class=\"f_CodeExample\">        key: dedicated.deckhouse.io</span><br/>\n<span class=\"f_CodeExample\">        operator: Equal</span><br/>\n<span class=\"f_CodeExample\">        value: frontend</span><br/>\n<span class=\"f_CodeExample\">    nodeSelector:</span><br/>\n<span class=\"f_CodeExample\">      kubernetes.io/os: linux</span><br/>\n<span class=\"f_CodeExample\">      node-role.deckhouse.io/frontend: \"\"</span><br/>\n<span class=\"f_CodeExample\">  controller:</span><br/>\n<span class=\"f_CodeExample\">    webhookPort: 443</span><br/>\n<span class=\"f_CodeExample\">    tolerations:</span><br/>\n<span class=\"f_CodeExample\">      - key: CriticalAddonsOnly</span><br/>\n<span class=\"f_CodeExample\">        operator: Exists</span><br/>\n<span class=\"f_CodeExample\">      - effect: NoExecute</span><br/>\n<span class=\"f_CodeExample\">        key: dedicated.deckhouse.io</span><br/>\n<span class=\"f_CodeExample\">        operator: Equal</span><br/>\n<span class=\"f_CodeExample\">        value: frontend</span><br/>\n<span class=\"f_CodeExample\">    nodeSelector:</span><br/>\n<span class=\"f_CodeExample\">      kubernetes.io/os: linux</span><br/>\n<span class=\"f_CodeExample\">      node-role.deckhouse.io/frontend: \"\"</span><br/>\n<span class=\"f_CodeExample\">...</span></p>\n<p class=\"p_Normal\"><a class=\"dropdown-toggle\" href=\"javascript:HMToggle('toggle','TOGGLE0186A5')\" style=\"font-style: normal; font-weight: normal; color: #000000; background-color: transparent; text-decoration: none;\">Заполнение параметров подключения к приватному registry для установки в закрытом контуре без доступа в интернет</a></p>\n<div class=\"dropdown-toggle-body\" id=\"TOGGLE0186A5\" style=\"text-align: left; text-indent: 0; padding: 0 0 0 0; margin: 0 0 0 0;\"><table style=\"border:none; border-spacing:0;\">\n<tr>\n<td style=\"vertical-align:top; padding:0; border:none\"><p class=\"p_Normal\"> <br/>\nДля подключения к приватному <span style=\"font-weight: bold;\">registry</span> необходимо:</p>\n<ol style=\"list-style-type:decimal\">\n<li class=\"p_Normal\" value=\"1\">Задать адрес и путь для параметра <code><b>openelb.global.imageRegistry</b></code>.</li></ol>\n<ol start=\"2\" style=\"list-style-type:decimal\">\n<li class=\"p_Normal\" value=\"2\">Указать наименование секрета с правами доступа к приватному <span style=\"font-weight: bold;\">registry</span> в параметре <code><b>imagePullSecrets</b></code>. Секрет должен быть создан вручную и зашифрован в Base64.</li></ol>\n<p class=\"p_CodeExample\" style=\"white-space: normal; page-break-inside: avoid;\"><span class=\"f_CodeExample\">## Настройки openelb</span><br/>\n<span class=\"f_CodeExample\">openelb:</span><br/>\n<span class=\"f_CodeExample\">   ## параметры подключения к приватному registry</span><br/>\n<span class=\"f_CodeExample\"> global:</span><br/>\n<span class=\"f_CodeExample\">     ## адрес и путь для приватного registry</span><br/>\n<span class=\"f_CodeExample\">   imageRegistry: registry.example.com/docker/addons</span><br/>\n<span class=\"f_CodeExample\">   ## секрет с правами доступа к приватному registry должен быть создан вручную, зашифрованный в Base64</span><br/>\n<span class=\"f_CodeExample\">   imagePullSecrets:</span><br/>\n<span class=\"f_CodeExample\">     - name: myRegistryKeySecretName</span></p>\n<p class=\"p_Normal\">где формат <code><b>imageRegistry</b></code>: адрес — <code><b>registry.example.com</b></code>.</p>\n</td>\n</tr>\n</table>\n</div>\n<ol start=\"3\" style=\"list-style-type:upper-roman\">\n<li class=\"p_Normal\" value=\"3\">Установите OpenELB в Kubernetes.</li></ol>\n<p class=\"p_Normal\">Для установки через интернет:</p>\n<p class=\"p_CodeExample\" style=\"page-break-inside: avoid;\"><span class=\"f_CodeExample\">helm upgrade --install openelb elma365/openelb -f values-openelb.yaml -n openelb-system --create-namespace</span></p>\n<p class=\"p_Normal\"><a class=\"dropdown-toggle\" href=\"javascript:HMToggle('toggle','TOGGLE0186A6')\" style=\"font-style: normal; font-weight: normal; color: #000000; background-color: transparent; text-decoration: none;\">Для установки без доступа в интернет</a></p>\n<div class=\"dropdown-toggle-body\" id=\"TOGGLE0186A6\" style=\"text-align: left; text-indent: 0; padding: 0 0 0 0; margin: 0 0 0 0;\"><table style=\"border:none; border-spacing:0;\">\n<tr>\n<td style=\"vertical-align:top; padding:0; border:none\"><p class=\"p_Normal\"> <br/>\nПерейдите в каталог с загруженным чартом и выполните команду:<br/>\n </p>\n<p class=\"p_CodeExample\" style=\"page-break-inside: avoid;\"><span class=\"f_CodeExample\">helm upgrade --install openelb ./openelb -f values-openelb.yaml -n openelb-system --create-namespace</span></p>\n</td>\n</tr>\n</table>\n</div>\n<ol start=\"4\" style=\"list-style-type:upper-roman\">\n<li class=\"p_Normal\" value=\"4\">Настройте отказоустойчивость openelb-controller.</li></ol>\n<p class=\"p_Normal\">Для достижения отказоустойчивости увеличьте количество реплик openelb-controller, выполнив команду на ноде master 1:</p>\n<p class=\"p_CodeExample\" style=\"page-break-inside: avoid;\"><span class=\"f_CodeExample\">kubectl scale --replicas=2 deployment openelb-controller -n openelb-system</span></p>\n<p class=\"p_Normal\">Выполните следующую команду, чтобы проверить находится ли openelb-controller в состоянии <span style=\"font-weight: bold;\">READY:</span> <span style=\"font-weight: bold;\">1/1</span> и <span style=\"font-weight: bold;\">STATUS:</span> <span style=\"font-weight: bold;\">Running</span>. Если да, OpenELB успешно установлен.</p>\n<p class=\"p_CodeExample\" style=\"page-break-inside: avoid;\"><span class=\"f_CodeExample\">kubectl get po -n openelb-system</span></p>\n<ol start=\"5\" style=\"list-style-type:upper-roman\">\n<li class=\"p_Normal\" value=\"5\">Создайте пул IP-адресов для OpenELB.</li></ol>\n<p class=\"p_Normal\">Создайте на ноде master 1 файл <code><b>vip-eip.yaml</b></code> с описанием EIP‑объекта. Объект Eip функционирует как пул IP-адресов для OpenELB.</p>\n<p class=\"p_Normal\">Пример файла <code><b>vip-eip.yaml</b></code>:</p>\n<p class=\"p_CodeExample\" style=\"page-break-inside: avoid;\"><span class=\"f_CodeExample\">apiVersion: network.kubesphere.io/v1alpha2</span><br/>\n<span class=\"f_CodeExample\">kind: Eip</span><br/>\n<span class=\"f_CodeExample\">metadata:</span><br/>\n<span class=\"f_CodeExample\">  name: vip-eip</span><br/>\n<span class=\"f_CodeExample\">  annotations:</span><br/>\n<span class=\"f_CodeExample\">    eip.openelb.kubesphere.io/is-default-eip: \"true\"</span><br/>\n<span class=\"f_CodeExample\">spec:</span><br/>\n<span class=\"f_CodeExample\">  address: 192.168.1.13</span><br/>\n<span class=\"f_CodeExample\">  protocol: vip</span><br/>\n<span class=\"f_CodeExample\">  interface: ens18</span></p>\n<p class=\"p_Normal\">Примените файл <code><b>vip-eip.yaml</b></code> в Kubernetes, выполнив команду:</p>\n<p class=\"p_CodeExample\" style=\"page-break-inside: avoid;\"><span class=\"f_CodeExample\">kubectl apply -f vip-eip.yaml</span></p>\n<ol start=\"6\" style=\"list-style-type:upper-roman\">\n<li class=\"p_Normal\" value=\"6\">Переместите поды keepalived на frontend-узлы.</li></ol>\n<p class=\"p_Normal\">По умолчанию поды keepalived размещаются openelb-manager на worker-узлах. В рамках статьи поды keepalived нужно разместить на frontend‑узлах.</p>\n<p class=\"p_Normal\">Внесите изменения в <span style=\"font-weight: bold;\">DaemonSet openelb-keepalive-vip</span>, выполнив команду на ноде master 1:</p>\n<p class=\"p_CodeExample\" style=\"page-break-inside: avoid;\"><span class=\"f_CodeExample\">kubectl patch ds -n openelb-system openelb-keepalive-vip -p '{\"spec\":{\"template\":{\"spec\":{\"nodeSelector\":{\"kubernetes.io/os\":\"linux\",\"node-role.deckhouse.io/frontend\":\"\"},\"tolerations\":[{\"key\":\"dedicated.deckhouse.io\",\"value\":\"frontend\",\"effect\":\"NoExecute\"}]}}}}'</span></p>\n<p class=\"p_Normal\">Проверьте, что изменения, внесённые в <span style=\"font-weight: bold;\">DaemonSet openelb-keepalive-vip</span> применились и поды разместились на нодах frontend:</p>\n<p class=\"p_CodeExample\" style=\"page-break-inside: avoid;\"><span class=\"f_CodeExample\">kubectl get po -o wide -n openelb-system</span></p>\n<h2 class=\"p_Heading2\"><a class=\"hmanchor\" id=\"addition-ingress-nginx-controller\"></a><span class=\"f_Heading2\">Шаг 12. Добавление Ingress Nginx Controller - LoadBalancer</span></h2>\n<p class=\"p_Normal\">Deckhouse устанавливает и управляет NGINX Ingress Controller при помощи Custom Resources. Если узлов для размещения Ingress-контроллера больше одного, он устанавливается в отказоустойчивом режиме и учитывает все особенности реализации инфраструктуры облаков и bare metal, а также кластеров Kubernetes различных типов.</p>\n<ol style=\"list-style-type:upper-roman\">\n<li class=\"p_Normal\" value=\"1\">Создайте на ноде master 1 файл <code><b>ingress-nginx-controller.yml</b></code>, содержащий конфигурацию Ingress-контроллера.</li></ol>\n<p class=\"p_Normal\"><a class=\"dropdown-toggle\" href=\"javascript:HMToggle('toggle','TOGGLE0186A7')\" style=\"font-style: normal; font-weight: normal; color: #000000; background-color: transparent; text-decoration: none;\">Пример файла ingress-nginx-controller.yml</a> </p>\n<div class=\"dropdown-toggle-body\" id=\"TOGGLE0186A7\" style=\"text-align: left; text-indent: 0; padding: 0 0 0 0; margin: 0 0 0 0;\"><table style=\"border:none; border-spacing:0;\">\n<tr>\n<td style=\"vertical-align:top; padding:0; border:none\"><p class=\"p_CodeExample\" style=\"page-break-inside: avoid;\"><span class=\"f_CodeExample\"> </span><br/>\n<span class=\"f_CodeExample\"># секция, описывающая параметры nginx ingress controller</span><br/>\n<span class=\"f_CodeExample\"># используемая версия API Deckhouse</span><br/>\n<span class=\"f_CodeExample\">apiVersion: deckhouse.io/v1</span><br/>\n<span class=\"f_CodeExample\">kind: IngressNginxController</span><br/>\n<span class=\"f_CodeExample\">metadata:</span><br/>\n<span class=\"f_CodeExample\">  name: nginx</span><br/>\n<span class=\"f_CodeExample\">spec:</span><br/>\n<span class=\"f_CodeExample\">  # имя Ingress-класса для обслуживания Ingress NGINX controller</span><br/>\n<span class=\"f_CodeExample\">  ingressClass: nginx</span><br/>\n<span class=\"f_CodeExample\">  # способ поступления трафика из внешнего мира</span><br/>\n<span class=\"f_CodeExample\">  inlet: LoadBalancer</span><br/>\n<span class=\"f_CodeExample\"># Аннотация для OpenELB в сервис nginx-load-balancer</span><br/>\n<span class=\"f_CodeExample\">  loadBalancer:</span><br/>\n<span class=\"f_CodeExample\">    annotations:</span><br/>\n<span class=\"f_CodeExample\">      eip.openelb.kubesphere.io/v1alpha2: \"vip-eip\"</span><br/>\n<span class=\"f_CodeExample\">      lb.kubesphere.io/v1alpha1: \"openelb\"</span><br/>\n<span class=\"f_CodeExample\">  hostPort:</span><br/>\n<span class=\"f_CodeExample\">    httpPort: 80</span><br/>\n<span class=\"f_CodeExample\">    httpsPort: 443</span><br/>\n<span class=\"f_CodeExample\">  # описывает, на каких узлах будет находиться компонент</span><br/>\n<span class=\"f_CodeExample\">  nodeSelector:</span><br/>\n<span class=\"f_CodeExample\">    node-role.kubernetes.io/frontend: \"\"</span><br/>\n<span class=\"f_CodeExample\">  tolerations:</span><br/>\n<span class=\"f_CodeExample\">  - operator: Exists</span></p>\n</td>\n</tr>\n</table>\n</div>\n<ol start=\"2\" style=\"list-style-type:upper-roman\">\n<li class=\"p_Normal\" value=\"2\">Примените файл <code><b>ingress-nginx-controller.yml</b></code> в Kubernetes, выполнив команду:</li></ol>\n<p class=\"p_CodeExample\" style=\"page-break-inside: avoid;\"><span class=\"f_CodeExample\">kubectl create -f ingress-nginx-controller.yml</span></p>\n<p class=\"p_Normal\">После установки ingress-контроллера Deckhouse автоматически создаст сервис nginx-load-balancer в namespace <code><b>d8-ingress-nginx</b></code>, но не свяжет данный сервис с OpenELB. </p>\n<ol start=\"2\" style=\"list-style-type:upper-roman\">\n<li class=\"p_Normal\" value=\"3\">Проверьте, что изменения, внесённые в сервис nginx-load-balancer применились, в <span style=\"font-weight: bold;\">EXTERNAL-IP</span> появился IP-адрес 192.168.1.13. Для этого выполните следующую команду:</li></ol>\n<p class=\"p_CodeExample\" style=\"page-break-inside: avoid;\"><span class=\"f_CodeExample\">kubectl get svc -n d8-ingress-nginx</span></p>\n<h2 class=\"p_Heading2\"><a class=\"hmanchor\" id=\"addition-user\"></a><span class=\"f_Heading2\">Шаг 13. Добавление пользователя для доступа к веб-интерфейсу кластера</span></h2>\n<ol style=\"list-style-type:upper-roman\">\n<li class=\"p_Normal\" value=\"1\">Создайте на ноде master 1 файл <code><b>user.yml</b></code>, содержащий описание учётной записи пользователя и прав доступа.</li></ol>\n<p class=\"p_Normal\"><a class=\"dropdown-toggle\" href=\"javascript:HMToggle('toggle','TOGGLE0186A8')\" style=\"font-style: normal; font-weight: normal; color: #000000; background-color: transparent; text-decoration: none;\">Пример файла user.yml</a></p>\n<div class=\"dropdown-toggle-body\" id=\"TOGGLE0186A8\" style=\"text-align: left; text-indent: 0; padding: 0 0 0 0; margin: 0 0 0 0;\"><table style=\"border:none; border-spacing:0;\">\n<tr>\n<td style=\"vertical-align:top; padding:0; border:none\"><p class=\"p_CodeExample\" style=\"page-break-inside: avoid;\"><span class=\"f_CodeExample\"> </span><br/>\n<span class=\"f_CodeExample\">apiVersion: deckhouse.io/v1</span><br/>\n<span class=\"f_CodeExample\">kind: ClusterAuthorizationRule</span><br/>\n<span class=\"f_CodeExample\">metadata:</span><br/>\n<span class=\"f_CodeExample\">  name: admin</span><br/>\n<span class=\"f_CodeExample\">spec:</span><br/>\n<span class=\"f_CodeExample\">  # список учётных записей Kubernetes RBAC</span><br/>\n<span class=\"f_CodeExample\">  subjects:</span><br/>\n<span class=\"f_CodeExample\">  - kind: User</span><br/>\n<span class=\"f_CodeExample\">    name: admin@deckhouse.io</span><br/>\n<span class=\"f_CodeExample\">  # предустановленный шаблон уровня доступа</span><br/>\n<span class=\"f_CodeExample\">  accessLevel: SuperAdmin</span><br/>\n<span class=\"f_CodeExample\">  # разрешить пользователю делать kubectl port-forward</span><br/>\n<span class=\"f_CodeExample\">  portForwarding: </span><span class=\"f_CodeExample\" style=\"font-weight: bold;\">true</span><br/>\n<span class=\"f_CodeExample\">---</span><br/>\n<span class=\"f_CodeExample\"># секция, описывающая параметры статического пользователя</span><br/>\n<span class=\"f_CodeExample\"># используемая версия API Deckhouse</span><br/>\n<span class=\"f_CodeExample\">apiVersion: deckhouse.io/v1</span><br/>\n<span class=\"f_CodeExample\">kind: User</span><br/>\n<span class=\"f_CodeExample\">metadata:</span><br/>\n<span class=\"f_CodeExample\">  name: admin</span><br/>\n<span class=\"f_CodeExample\">spec:</span><br/>\n<span class=\"f_CodeExample\">  # e-mail пользователя</span><br/>\n<span class=\"f_CodeExample\">  email: admin@deckhouse.io</span><br/>\n<span class=\"f_CodeExample\">  # это хэш пароля xgnv5gkggd, сгенерированного сейчас</span><br/>\n<span class=\"f_CodeExample\">  # сгенерируйте свой или используйте этот, но только для тестирования</span><br/>\n<span class=\"f_CodeExample\">  # echo \"xgnv5gkggd\" | htpasswd -BinC 10 \"\" | cut -d: -f2</span><br/>\n<span class=\"f_CodeExample\">  # возможно, захотите изменить</span><br/>\n<span class=\"f_CodeExample\">  password: '$2a$10$4j4cUeyonCfX7aDJyqSHXuAxycsf/sDK0T4n9ySQ7.owE34L1uXTm'</span></p>\n</td>\n</tr>\n</table>\n</div>\n<ol start=\"2\" style=\"list-style-type:upper-roman\">\n<li class=\"p_Normal\" value=\"2\">Примените файл <code><b>user.yml</b></code> в Kubernetes, выполнив команду:</li></ol>\n<p class=\"p_CodeExample\" style=\"page-break-inside: avoid;\"><span class=\"f_CodeExample\">kubectl create -f user.yml</span></p>\n<h2 class=\"p_Heading2\"><a class=\"hmanchor\" id=\"load-privileges\"></a><span class=\"f_Heading2\">Шаг 14. Привилегии запускаемых нагрузок</span></h2>\n<p class=\"p_Normal\">Разрешите переназначить политику привилегий для запускаемых подов:</p>\n<p class=\"p_CodeExample\" style=\"page-break-inside: avoid;\"><span class=\"f_CodeExample\">kubectl label namespace elma365 security.deckhouse.io/pod-policy=privileged --overwrite</span></p>\n<h2 class=\"p_Heading2\"><span class=\"f_Heading2\"> </span></h2>\n<div class=\"bottom-nav\">\n<a class=\"topic__navi_prev\" href=\"kubernetes-deckhouse-air-gap.html\" id=\"prev-link\">\n<span class=\"bottom-nav__arrow bottom-nav__arrow--prev\"></span> <span class=\"bottom-nav__link\">kubernetes-deckhouse-air-gap.html</span>\n</a>\n<a class=\"topic__navi_next\" href=\"embedded-databases-settings.html\" id=\"next-link\">\n<span class=\"bottom-nav__link\">embedded-databases-settings.html</span> <span class=\"bottom-nav__arrow bottom-nav__arrow--next\"></span>\n</a>\n</div>\n<!-- добавляет на страницу строку блок Была ли статья полезной?  -->\n<div class=\"feedback-wrap\"><div class=\"feedback\" id=\"feedback\"><span><b>Была ли статья полезной?</b></span><form action=\"\" class=\"feedback-form\" id=\"feedback-form\" method=\"POST\"><div class=\"feedback__popup feedback__popup-response\" id=\"feedback__popup_thx\">Спасибо за ваш отзыв!</div><div id=\"feedback-success-popup\"><div class=\"wrap\"><button class=\"feedback-popup-close\" type=\"button\">×</button><svg fill=\"none\" height=\"44\" viewbox=\"0 0 44 44\" width=\"44\" xmlns=\"http://www.w3.org/2000/svg\"><g clip-path=\"url(#clip0_212_2187)\"><path d=\"M22 0.6875C10.2294 0.6875 0.6875 10.2294 0.6875 22C0.6875 33.7706 10.2294 43.3125 22 43.3125C33.7706 43.3125 43.3125 33.7706 43.3125 22C43.3125 10.2294 33.7706 0.6875 22 0.6875ZM22 40.5625C11.8023 40.5625 3.4375 32.3078 3.4375 22C3.4375 11.8024 11.6922 3.4375 22 3.4375C32.1977 3.4375 40.5625 11.6922 40.5625 22C40.5625 32.1976 32.3078 40.5625 22 40.5625ZM34.1713 16.933L18.6613 32.3186C18.257 32.7197 17.604 32.7171 17.203 32.3128L9.82283 24.873C9.42176 24.4686 9.42434 23.8157 9.82867 23.4146L10.5609 22.6884C10.9652 22.2873 11.6181 22.2899 12.0192 22.6942L17.9468 28.6697L31.9926 14.7366C32.3969 14.3356 33.0498 14.3382 33.4509 14.7425L34.1772 15.4747C34.5783 15.879 34.5757 16.532 34.1713 16.933Z\" fill=\"#27AE60\"></path></g><defs><clippath id=\"clip0_212_2187\"><rect fill=\"white\" height=\"44\" width=\"44\"></rect></clippath></defs></svg><p>Ваш отзыв успешно отправлен!</p><span>Спасибо за обратную связь.</span></div></div><div class=\"feedback__popup\" id=\"feedback__popup_why\"><button class=\"feedback-popup-close\" type=\"button\">×</button><div class=\"feedback__popup-header\">Уточните, почему:</div><input id=\"bad_recommendation\" name=\"category\" type=\"radio\" value=\"bad_recommendation\"/><label for=\"bad_recommendation\">Рекомендации не помогли</label><input id=\"difficult_text\" name=\"category\" type=\"radio\" value=\"difficult_text\"/><label for=\"difficult_text\">Текст трудно понять</label><input id=\"no_answer\" name=\"category\" type=\"radio\" value=\"no_answer\"/><label for=\"no_answer\">Нет ответа на мой вопрос</label><input id=\"bad_header\" name=\"category\" type=\"radio\" value=\"bad_header\"/><label for=\"bad_header\">Содержание статьи не соответствует заголовку</label><input id=\"other_reason\" name=\"category\" type=\"radio\" value=\"other_reason\"/><label for=\"other_reason\">Другая причина</label></div><div class=\"feedback__popup\" id=\"feedback__popup-other\"><button class=\"feedback-popup-close\" type=\"button\">×</button> <div class=\"feedback__popup-header\">Расскажите, что вам не понравилось в статье:</div><textarea class=\"feedback__textarea\" id=\"\" name=\"other\"></textarea><input class=\"feedback__other-btn\" type=\"submit\" value=\"Отправить\"/></div><div class=\"feedback-form__btn-group\"><input id=\"feedback__useful_yes\" name=\"useful\" type=\"radio\" value=\"true\"/><label for=\"feedback__useful_yes\"><img src=\"like.svg\"/><span class=\"feedback-form__btn-group_yes-btn\">Да</span></label><input id=\"feedback__useful_no\" name=\"useful\" type=\"radio\" value=\"false\"/><label for=\"feedback__useful_no\"><img src=\"dislike.svg\"/><span class=\"feedback-form__btn-group_no-btn\">Нет</span></label></div><select name=\"category\"><option disabled=\"\">Выберите вариант</option><option selected=\"\" value=\"bad_recommendation\">Рекомендации не помогли</option><option value=\"difficult_text\">Текст трудно понять</option><option value=\"no_answer\">Нет ответа на мой вопрос</option><option value=\"bad_header\">Содержание статьи не соответствует заголовку</option><option value=\"other_reason\">Другая причина</option></select><input type=\"submit\"/></form></div></div>\n</section>\n</div>\n<aside class=\"article__sidebar\" style=\"display:none\">\n<input type=\"checkbox\"/>\n<div class=\"article__arrow\"></div>\n<div class=\"table-of-contents elma365-right\" id=\"toc2Content\">\n<h3 class=\"h3-toc\">В этой статье</h3>\n<nav id=\"toc2\"></nav>\n</div>\n</aside>\n</div>\n</article>\n</main>\n<footer class=\"footer\">\n<div class=\"footer-container\">\n<div class=\"footer-mobile\">\n<ul class=\"footer-mobile__list\"><li><a href=\"https://api.elma365.com/ru/\" target=\"_blank\">API</a></li><li><a href=\"https://tssdk.elma365.com/\" target=\"_blank\">TS SDK</a></li><li><a href=\"https://community.elma365.com/\" target=\"_blank\">Community</a></li><li><a href=\"https://elma-academy.com/ru/elma365\" target=\"_blank\">Академия</a></li></ul><ul class=\"footer-mobile__list\"><li><a href=\"https://elma365.com/ru/help/platform/get-trial.html\">Платформа</a></li><li><a href=\"https://elma365.com/ru/help/ecm/ecm-functions.html\">ECM</a></li><li><a href=\"https://elma365.com/ru/help/service/service-functions.html\">Service</a></li><li><a href=\"https://elma365.com/ru/help/projects/projects-functions.html\">Проекты</a></li></ul>\n</div>\n<div class=\"container\">\n<div class=\"footer-wrap\">\n<div><span class=\"mobile-question-popup\">Отправить фидбэк</span><form action=\"\" class=\"question__popup question-xs\" id=\"question__popup\" method=\"POST\"><div class=\"question-wrap\"><span class=\"close\"></span><span class=\"title\">Задать вопрос</span><label for=\"help_question\" style=\"display: none;\"></label><textarea id=\"help_question\" name=\"help_question\"></textarea><input type=\"submit\" value=\"Отправить\"/></div></form><div class=\"hidden fade-in question-success-xs\">Ваш фидбэк отправлен.</div></div>\n<div class=\"footer-flex-b\">\n<div class=\"footer-top\">\n<span class=\"footer-copy\">© 2025 \n              ELMA365\n            \n            \n          </span>\n<a class=\"sk-img\" href=\"https://navigator.sk.ru/orn/1122971\" target=\"_blank\">\n<img alt=\"sk icon\" class=\"footer-img\" height=\"34\" src=\"sk-resident.svg\" width=\"117\"/>\n</a>\n</div>\n<div class=\"footer-line\">\n<div class=\"footer-line-copy\">\n<span class=\"footer-copy\">© 2025 \n                  ELMA365\n                \n                \n              </span>\n</div>\n<ul class=\"footer-list\">\n<li class=\"footer-item footer-url-elma\"><a class=\"footer-link footer-url-link-elma\" href=\"https://elma365.com/ru/\" style=\"color: #0D4A75;\" target=\"_blank\"><img alt=\"browse icon\" class=\"footer-img footer-img-elma-url\" src=\"browse.svg\"/>elma365.com</a></li><li class=\"footer-item\"><a class=\"footer-link\" href=\"https://vkvideo.ru/@elma_bpm\" target=\"_blank\"><img alt=\"vk-video icon\" class=\"footer-img social-footer-img\" height=\"20\" src=\"vk-video.svg\" width=\"20\"/></a></li><li class=\"footer-item\"><a class=\"footer-link\" href=\"https://vk.com/elma_bpm\" target=\"_blank\"><img alt=\"vk icon\" class=\"footer-img social-footer-img\" height=\"20\" src=\"vk.svg\" width=\"20\"/></a></li><li class=\"footer-item\"><a class=\"footer-link\" href=\"https://t.me/elmaday\" target=\"_blank\"><img alt=\"telegram icon\" class=\"footer-img social-footer-img\" height=\"20\" src=\"telegram.svg\" width=\"20\"/></a></li><li class=\"footer-item\"><a class=\"footer-link\" href=\"https://dzen.ru/elma\" target=\"_blank\"><img alt=\"dzen icon\" class=\"footer-img social-footer-img\" height=\"20\" src=\"social_dzen.svg\" width=\"20\"/></a></li>\n<li class=\"footer-item sk-footer-img\">\n<a class=\"sk-img\" href=\"https://navigator.sk.ru/orn/1122971\" target=\"_blank\">\n<img alt=\"sk icon\" class=\"footer-img\" height=\"34\" src=\"sk-resident.svg\" width=\"117\"/>\n</a>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>\n<a class=\"arrow-top\" href=\"#\"></a>\n</div>\n</footer>\n<!--    <script type=\"text/javascript\" src=\"jquery1.min.js\"></script>-->\n<iframe name=\"hmnavigation\" style=\"display:none!important\"></iframe>\n<!--<script src=\"./jquery-ui.js\"></script> -->\n<script src=\"./jquery-ui.min.js\"></script>\n<!--script src=\"//cdn.jsdelivr.net/npm/featherlight@1.7.14/release/featherlight.min.js\" type=\"text/javascript\" charset=\"utf-8\"></script-->\n<script src=\"./jquery.tocify.min.js\"></script>\n<script src=\"./TypoReporter.min.js\"></script>\n<script src=\"./google-search.js\"></script>\n<script src=\"./main.js\"></script>\n<script type=\"text/javascript\">\nHMInitToggle('TOGGLE0186A1','hm.type','dropdown','hm.state','0');\nHMInitToggle('TOGGLE0186A2','hm.type','dropdown','hm.state','0');\nHMInitToggle('TOGGLE0186A3','hm.type','dropdown','hm.state','0');\nHMInitToggle('TOGGLE0186A4','hm.type','dropdown','hm.state','0');\nHMInitToggle('TOGGLE0186A5','hm.type','dropdown','hm.state','0');\nHMInitToggle('TOGGLE0186A6','hm.type','dropdown','hm.state','0');\nHMInitToggle('TOGGLE0186A7','hm.type','dropdown','hm.state','0');\nHMInitToggle('TOGGLE0186A8','hm.type','dropdown','hm.state','0');\n</script>\n</body>\n</html>\n",
  "plain_text": "Отказоустойчивый Kubernetes-кластер ﻿ Платформа CSP CRM+CX Проекты Service КЭДО Бизнес-решения Academy Community API SDK Сайт ELMA365 ELMA365 On-Premises > Подготовка инфраструктуры ELMA365 On-Premises > Kubernetes / Отказоустойчивый Kubernetes-кластер Отказоустойчивый Kubernetes-кластер Deckhouse — это полнофункциональная платформа на базе Open Source-компонентов, которая, кроме Kubernetes, включает дополнительные модули для мониторинга, балансировки трафика, автомасштабирования, безопасного доступа и не только. Модули преднастроены, интегрированы друг с другом и готовы к работе. Управление всеми компонентами кластера и платформы, а также их обновление полностью автоматизированы. Deckhouse в реестре российского ПО и сертифицирован в CNCF . Установка состоит из следующих этапов: Архитектура кластера Kubernetes . Рекомендуемые системные требования . Подготовка конфигурационного файла . Установка Kubernetes-кластера на базе Deckhouse . Добавление frontend-узлов . Добавление system-узлов . Добавление worker-узлов . Добавление master-узлов . Добавление Local Path Provisioner . Установка HELM . Добавление балансировщика OpenELB - VIP . Добавление Ingress Nginx Controller - LoadBalancer . Добавление пользователя для доступа к веб-интерфейсу кластера . Привилегии запускаемых нагрузок . Шаг 1. Архитектура кластера Kubernetes В рамках статьи рассматривается внедрение инфраструктуры отказоустойчивого кластера Kubernetes на базе платформы Deckhouse. Структура Kubernetes-кластера. Чтобы развернуть минимальную структуру Kubernetes-кластера на базе платформы Deckhouse, потребуются: персональный компьютер; master-узел — три ноды; worker-узел — три ноды; system-узел — две ноды; frontend-узел — две ноды. В рассматриваемом примере web‑трафик от пользователей поступает на виртуальный IP‑адрес 192.168.1.13, размещённый на frontend-нодах. Шаблон доменного имени для доступа к web-сервисам платформы Deckhouse выберите %s.example.com . Deckhouse автоматически настраивает и управляет узлами кластера и его компонентами control plane , постоянно поддерживая их актуальную конфигурацию. При развёртывании master-узлов автоматически создаются все необходимые компоненты для control plane с помощью модуля control-plane-manager . Deckhouse создаёт сущности Kubernetes по мере необходимости и так же их удаляет. Например, если в вашем кластере нет frontend-нод и с master-нод не снято ограничение taint , вы не сможете установить IngressNginxController . В кластере будут отсутствовать необходимые сущности, как ingressClass и так далее. При добавлении system-нод Deckhouse автоматически развернёт компоненты мониторинга и web-сервисы для доступа к интерфейсу платформы. Web-сервисы автоматически привяжутся на %s.example.com . Загрузка образов Deckhouse в локальный реестр образов. Кластер Kubernetes с помощью Deckhouse можно развернуть в закрытом окружении, из которого нет доступа в интернет. Для этого предварительно скачайте на компьютере с доступом в интернет образы платформы Deckhouse и загрузите их в локальный реестр образов. Подробнее читайте в статье «Загрузка образов Deckhouse» . Шаг 2. Рекомендуемые системные требования Персональный компьютер. Компьютер, с которого будет производиться установка. Он нужен только для запуска инсталлятора Deckhouse и не будет частью кластера. Требования: ОС: Windows 10+, macOS 10.15+, Linux (Ubuntu 18.04+, Fedora 35+); установленный docker для запуска инсталлятора Deckhouse; доступ до проксирующего registry или до частного хранилища образов контейнеров с образами контейнеров Deckhouse; SSH-доступ по ключу до узла, который будет master-узлом будущего кластера. Ноды Kubernetes: поддерживаемая ОС ; конфигурация нод: Наименование vCPU RAM (GB) Жесткий диск (GB) LAN (Gbit/s) Kubernetes worker 8 16 60 1 Kubernetes system 8 16 200 1 Kubernetes master 4 8 60 1 Kubernetes frontend 4 6 60 1 доступ до проксирующего registry или до частного хранилища образов контейнеров с образами контейнеров Deckhouse; Начало внимание Deckhouse поддерживает работу только с Bearer token схемой авторизации в registry . Конец внимание доступ до прокси-сервера для скачивания deb/rpm-пакетов ОС при необходимости; на узле не должно быть установлено пакетов container runtime, например, containerd или docker. Начало примечание Примечание Установка непосредственно с master-узла в настоящий момент не поддерживается. Установщик в виде Docker-образа нельзя запускать на том же узле, на котором планируется развёртывание master-узла, так как на узле не должно быть установлено пакетов container runtime, например, containerd или docker. При отсутствии менеджмент узлов установите docker на любой другой ноде будущего кластера, запустите Docker-образ установщика, установите Deckhouse и удалите Docker-образ установщика c ноды вместе с docker. Конец примечание Шаг 3. Подготовка конфигурационного файла Чтобы установить Deckhouse, подготовьте YAML-файл конфигурации установки. Для получения YAML-файла конфигурации воспользуйтесь сервисом Быстрый старт на сайте Deckhouse. Сервис сгенерирует актуальный YAML-файл для текущей версий платформы. Сгенерируйте YAML-файл сервисом Быстрый старт , выполнив следующие шаги: Выберите инфраструктуру — Bare Metal. Ознакомьтесь с информацией об установке. Укажите шаблон для DNS-имён кластера. В нашем случае — %s.example.com . Сохраните config.yml . Внесите необходимые изменения в config.yml . Для этого выполните следующие действия: Задайте адресное пространство подов кластера в podSubnetCIDR . Задайте адресное пространство Service’ов кластера в serviceSubnetCIDR . Задайте нужную версию Kubernetes в kubernetesVersion . Проверьте канал обновления в releaseChannel (EarlyAccess). Проверьте шаблон доменного имени в publicDomainTemplate ( %s.example.com ). Используется для формирования доменов системных приложений в кластере. Например, Grafana для шаблона %s.example.com будет доступна, как grafana.example.com . Проверьте режим работы модуля cni-flannel в podNetworkMode . Режим работы flannel, допустимые значения VXLAN (если ваши сервера имеют связность L3) или HostGW (для L2-сетей). Укажите локальную сеть, которую будут использовать узлы кластера в internalNetworkCIDRs . Список внутренних сетей узлов кластера, например, '192.168.1.0/24' , который используется для связи компонентов Kubernetes (kube-apiserver, kubelet и т. д.) между собой. Пример файла первичной конфигурации кластера — config.yml . Для установки через интернет : apiVersion: deckhouse.io/v1 kind: ClusterConfiguration clusterType: Static podSubnetCIDR: 10.111.0.0/16 serviceSubnetCIDR: 10.222.0.0/16 kubernetesVersion: \"Automatic\" clusterDomain: \"cluster.local\" --- apiVersion: deckhouse.io/v1 kind: InitConfiguration deckhouse: imagesRepo: registry.deckhouse.ru/deckhouse/ce registryDockerCfg: eyJhdXRocyI6IHsgInJlZ2lzdHJ5LmRlY2tob3VzZS5ydSI6IHt9fX0K --- apiVersion: deckhouse.io/v1alpha1 kind: ModuleConfig metadata: name: deckhouse spec: version: 1 enabled: true settings: bundle: Default releaseChannel: EarlyAccess logLevel: Info --- apiVersion: deckhouse.io/v1alpha1 kind: ModuleConfig metadata: name: global spec: version: 2 settings: modules: publicDomainTemplate: \"%s.elewise.local\" --- apiVersion: deckhouse.io/v1alpha1 kind: ModuleConfig metadata: name: user-authn spec: version: 2 enabled: true settings: controlPlaneConfigurator: dexCAMode: DoNotNeed publishAPI: enabled: true https: mode: Global global: kubeconfigGeneratorMasterCA: \"\" --- apiVersion: deckhouse.io/v1alpha1 kind: ModuleConfig metadata: name: cni-cilium spec: version: 1 enabled: true settings: tunnelMode: VXLAN --- apiVersion: deckhouse.io/v1 kind: StaticClusterConfiguration internalNetworkCIDRs: - 192.168.1.0/24 Для офлайн-установки без доступа в интернет Начало внимание Для генерации YAML-файла сервисом Быстрый старт выберите инфраструктуру — Закрытое окружение . Конец внимание Установите следующие параметры в ресурсе InitConfiguration : devBranch : если в изолированном приватном репозитории нет образов, содержащих информацию о каналах обновлений, используйте точный тег образа Deckhouse, чтобы установить Deckhouse Platform. Например, если вы хотите установить релиз v1.46.3, используйте образ registry.example.com/images/deckhouse/install:v1.46.3 . Также укажите devBranch: v1.46.3 вместо releaseChannel: XXX ; imagesRepo : <PROXY_REGISTRY>/<DECKHOUSE_REPO_PATH>/<DECKHOUSE_REVISION> — адрес образа Deckhouse в приватном репозитории с учётом используемой редакции. В рамках статьи образы были загружены в registry.example.com/images/deckhouse . Подробнее читайте в статье «Загрузка образов Deckhouse» ; registryDockerCfg : <BASE64> — права доступа к приватному репозиторию, зашифрованные в Base64. Примеры заполнения registryDockerCfg читайте в официальной документации Deckhouse Kubernetes Platform . В рамках статьи разрешён анонимный доступ к образам Deckhouse в стороннем registry . Сформируйте registryDockerCfg , выполнив следующую команду: echo -n \"{\\\"auths\\\": { \\\"registry.example.com:443/images/deckhouse\\\": {}}}\" | base64 registryScheme : укажите, по какому протоколу (HTTP, HTTPS) работает приватный репозиторий; registryCA : корневой SSL-сертификат, которым можно проверить SSL-сертификат приватного реестра, например, если хранилище использует самоподписанный сертификат. Если вы используете не самоподписанный сертификат или хранилище работает по протоколу HTTP, удалите этот параметр. Пример файла первичной конфигурации кластера — config.yml : apiVersion: deckhouse.io/v1 kind: ClusterConfiguration clusterType: Static podSubnetCIDR: 10.111.0.0/16 serviceSubnetCIDR: 10.222.0.0/16 kubernetesVersion: \" Automatic \" clusterDomain: \"cluster.local\" --- apiVersion: deckhouse.io/v1 kind: InitConfiguration deckhouse: devBranch: v1.46.3 configOverrides: global: modules: publicDomainTemplate: \"%s.example.com\" cniFlannelEnabled: true cniFlannel: podNetworkMode: VXLAN imagesRepo: registry.example.com:443/images/deckhouse registryDockerCfg: eyJhdXRocyI6IHsgInJlZ2lzdHJ5LmV4YW1wbGUuY29tOjQ0My9pbWFnZXMvZGVja2hvdXNlIjoge319fQ== registryScheme: HTTPS registryCA: | -----BEGIN CERTIFICATE----- MIIFBzCCGu+gAwIBAgIUBZ37mm02QGGcmd5pZvWwnpCfQUowDQYGKoZIhvcNAQEL BQAwHjEcMBoGA1UEAwwTaW1hZ2VzLnByb2FjdG9yLnBybzAeFw0yMjA5MjkxNDUw ... 9UpckrwxPhctmln5/Awd/2gcaRAxI3qBL7SyDFT0YpnGcAiGPY4Z67HhZ7h6y+2F fQDSXli0r61/Fenkh5OLMihLYTm+5gjZlG1LCXpaGIpjAf16Q+3/pIqapQ== -----END CERTIFICATE----- --- apiVersion: deckhouse.io/v1 kind: StaticClusterConfiguration internalNetworkCIDRs: - 192.168.1.0/24 Шаг 4. Установка Kubernetes-кластера на базе Deckhouse Установка Deckhouse Platform Community Edition сводится к установке кластера, который состоит из единственного master-узла. Инсталлятор Deckhouse доступен в виде образа контейнера, в который необходимо передать конфигурационные файлы и SSH-ключи доступа на master-узел. Далее подразумевается, что используется SSH-ключ ~/.ssh/id_rsa . В основе инсталлятора лежит утилита dhctl . Запустите установщик. Начало примечание Примечание Установка непосредственно с master-узла в настоящий момент не поддерживается. Установщик в виде Docker-образа нельзя запускать на том же узле, на котором планируется развёртывание master-узла, так как на узле не должно быть установлено пакетов container runtime, например, containerd или docker. Конец примечание Установщик запускается на персональном компьютере, подготовленном на этапе архитектура кластера Kubernetes . На ПК перейдите в директорию с файлом конфигурации config.yml , подготовленным на этапе подготовка конфигурационного файла . Для запуска установщика через интернет выполните команду: sudo docker run --pull=always -it -v \"$PWD/config.yml:/config.yml\" -v \"$HOME/.ssh/:/tmp/.ssh/\" registry.deckhouse.io/deckhouse/ce/install:stable bash Для офлайн-установки без доступа в интернет Выполните команду: sudo docker run --pull=always -it -v \"$PWD/config.yml:/config.yml\" -v \"$HOME/.ssh/:/tmp/.ssh/\" example.com:443/images/deckhouse/install:v1.46.3 bash Где: example.com:443/images/deckhouse/install:v1.46.3 — версия устанавливаемого релиза. Установите Deckhouse. Для этого внутри контейнера установщика выполните команду: dhctl bootstrap --ssh-user=<username> --ssh-host=<master_ip> --ssh-agent- private -keys=/tmp/.ssh/id_rsa \\ --config=/config.yml \\ --ask-become-pass где: <username> — в параметре --ssh-user укажите имя пользователя, от которого генерировался SSH-ключ для установки; <master_ip> — IP адрес master-узла подготовленного на этапе архитектура кластера Kubernetes . Процесс установки может занять 15-30 минут при хорошем соединении. Шаг 5. Добавление frontend-узлов Перед добавлением frontend-нод предварительно создайте новый custom resource NodeGroup с именем frontend . Параметр nodeType в custom resource NodeGroup задайте как Static . Создайте на ноде master 1 файл frontend.yaml с описанием статичной NodeGroup с наименованием frontend : apiVersion: deckhouse.io/v1 kind: NodeGroup metadata: name: frontend spec: nodeTemplate: labels: node-role.deckhouse.io/frontend: \"\" taints: - effect: NoExecute key: dedicated.deckhouse.io value: frontend nodeType: Static Примените файл frontend.yaml , выполнив команду: kubectl create -f frontend.yaml Для добавления frontend-нод выполните следующие действия: Получите код скрипта в кодировке Base64 для добавления и настройки нового узла frontend, выполнив команду на ноде master 1: kubectl -n d8-cloud-instance-manager get secret manual-bootstrap- for -frontend -o json | jq '.data.\"bootstrap.sh\"' -r Зайдите на узел, который хотите добавить по SSH (в данном случае frontend 1) и вставьте полученную на первом шаге Base64‑строку: echo <Base64-КОД-СКРИПТА> | base64 -d | bash Дождитесь окончания выполнения скрипта. Нода добавлена. Чтобы добавить новые frontend-ноды, выполните действия из пункта 3. Шаг 6. Добавление system-узлов Перед добавлением system-нод предварительно создайте новый custom resource NodeGroup с именем system . Параметр nodeType в custom resource NodeGroup задать как Static . Создайте на ноде master 1 файл system.yaml с описанием статичной NodeGroup с наименованием system : apiVersion: deckhouse.io/v1 kind: NodeGroup metadata: name: system spec: nodeTemplate: labels: node-role.deckhouse.io/system: \"\" taints: - effect: NoExecute key: dedicated.deckhouse.io value: system nodeType: Static Примените файл system.yaml , выполнив команду: kubectl create -f system.yaml Для добавления system-нод выполните следующие действия: Получите код скрипта в кодировке Base64 для добавления и настройки нового узла system, выполнив команду на ноде master 1: kubectl -n d8-cloud-instance-manager get secret manual-bootstrap- for -system -o json | jq '.data.\"bootstrap.sh\"' -r Зайдите на узел, который хотите добавить по SSH (в данном случае system 1) и вставьте полученную на первом шаге Base64‑строку: echo <Base64-КОД-СКРИПТА> | base64 -d | bash Дождитесь окончания выполнения скрипта. Нода добавлена. Чтобы добавить новые system-ноды, выполните действия из пункта 3. Шаг 7. Добавление worker-узлов Перед добавлением worker-нод предварительно создайте новый custom resource NodeGroup с именем worker . Параметр nodeType в custom resource NodeGroup задать как Static . Создайте на ноде master 1 файл worker.yaml с описанием статичной NodeGroup с наименованием worker : apiVersion: deckhouse.io/v1 kind: NodeGroup metadata: name: worker spec: nodeType: Static kubelet: maxPods: 200 Примените файл worker.yaml , выполнив команду: kubectl create -f worker.yaml Для добавления worker-нод выполните следующие действия: Получите код скрипта в кодировке Base64 для добавления и настройки нового узла worker, выполнив команду на ноде master 1: kubectl -n d8-cloud-instance-manager get secret manual-bootstrap- for -worker -o json | jq '.data.\"bootstrap.sh\"' -r Зайдите на узел, который хотите добавить по SSH (в данном случае worker 1) и вставьте полученную на первом шаге Base64-строку: echo <Base64-КОД-СКРИПТА> | base64 -d | bash Дождитесь окончания выполнения скрипта. Нода добавлена. Чтобы добавить новые worker-ноды, выполните действия из пункта 3. Шаг 8. Добавление master-узлов Получите код скрипта в кодировке Base64 для добавления и настройки нового узла master, выполнив команду на ноде master 1: kubectl -n d8-cloud-instance-manager get secret manual-bootstrap- for -master -o json | jq '.data.\"bootstrap.sh\"' -r Зайдите на узел, который хотите добавить по SSH (в данном случае master 2) и вставьте полученную на первом шаге Base64-строку: echo <Base64-КОД-СКРИПТА> | base64 -d | bash Дождитесь окончания выполнения скрипта. Нода добавлена. Чтобы добавить новые master-ноды, выполните действия из шага 8. Шаг 9. Добавление Local Path Provisioner По умолчанию storageclass отсутствует в Deckhouse. Создайте custom resource LocalPathProvisioner , позволяющий пользователям Kubernetes использовать локальное хранилище на узлах. Для этого выполните следующие действия: Создайте на master-узле файл local-path-provisioner.yaml , содержащий конфигурацию для LocalPathProvisioner . Установите нужную Reclaim policy (по умолчанию устанавливается Retain). В рамках статьи для параметра reclaimPolicy установлено \"Delete\" (PV после удаления PVC удаляются). Пример файла local-path-provisioner.yaml : apiVersion: deckhouse.io/v1alpha1 kind: LocalPathProvisioner metadata: name: localpath-deckhouse-system spec: nodeGroups: - system - worker path: \"/opt/local-path-provisioner\" reclaimPolicy: \"Delete\" Примените файл local-path-provisioner.yaml в Kubernetes. Для этого выполните команду: kubectl apply -f local-path-provisioner.yaml Установите созданный LocalPathProvisioner , как storageclass по умолчанию (default-class), выполнив одну из команд: kubectl patch storageclass localpath-deckhouse-system -p '{\"metadata\": {\"annotations\":{\"storageclass.kubernetes.io/is-default-class\":\"true\"}}}' или sudo -i d8 k patch mc global --type merge -p “{\"spec\": {\"settings\":{\"defaultClusterStorageClass\":\"localpath-deckhouse-system\"}}}” LocalPathProvisioner с наименованием localpath-deckhouse-system создан и готов предоставлять локальные хранилища на узлах c NodeGroup system и worker. Шаг 10. Установка HELM Перейдите на страницу релизов Helm и скачайте архив helm-vX.Y.Z-linux-amd64.tar.gz нужно версии. Для установки через интернет: wget https://get.helm.sh/helm-vX.Y.Z-linux-amd64.tar.gz Для офлайн-установки без доступа в интернет На компьютере с доступом в интернет перейдите на страницу релизов Helm и скачайте архив helm-vX.Y.Z-linux-amd64.tar.gz нужной версии, выполнив команду: wget https://get.helm.sh/helm-vX.Y.Z-linux-amd64.tar.gz Скопируйте полученный архив на master-узел. Распакуйте архив и переместите бинарный файл helm: tar -zxvf helm-vX.Y.Z-linux-amd64.tar.gz mv linux-amd64/helm /usr/local/bin/helm Шаг 11. Добавление балансировщика OpenELB-VIP Для правильной работы Ingress-контроллера требуется прямой выход в интернет с белым IP-адресом на Ingress-узле кластера с использованием NodePort, или можно установить балансировщик OpenELB , который будет заниматься балансировкой трафика так, как это организовано у любого облачного провайдера. Этот балансировщик использует Speaker для поддержки IP-адреса службы. Разверните OpenELB в режиме VIP Mode. Для этого: Получите конфигурационный файл values-openelb.yaml . Для установки через интернет: helm repo add elma365 https: //charts.elma365.tech helm repo update helm show values elma365/openelb > values-openelb.yaml Получение конфигурационного файла для установки в закрытом контуре без доступа в интернет На компьютере с доступом в интернет скачайте архив актуальной версии (latest) чарта OpenELB из репозитория elma365, выполнив следующую команду: helm repo add elma365 https: //charts.elma365.tech helm repo update helm pull elma365/openelb Полученный архив чарта openelb-X.Y.Z.tgz скопируйте на сервер, где будет производиться установка. Распакуйте чарт openelb-X.Y.Z.tgz на сервере, где будет производиться установка и скопируйте конфигурационный файл по умолчанию values.yaml в values-openelb.yaml . tar -xf openelb-X.Y.Z.tgz cp openelb/values.yaml values-openelb.yaml Измените конфигурационный файл values-openelb.yaml . Запланируйте размещение подов openelb-controller на frontend-нодах кластера. Для этого измените секции tolerations и nodeSelector : ## Настройки openelb openelb: speaker: enable: true vip: true apiHosts: \":50051\" tolerations: - key: CriticalAddonsOnly operator: Exists - effect: NoExecute key: dedicated.deckhouse.io operator: Equal value: frontend nodeSelector: kubernetes.io/os: linux node-role.deckhouse.io/frontend: \"\" controller: webhookPort: 443 tolerations: - key: CriticalAddonsOnly operator: Exists - effect: NoExecute key: dedicated.deckhouse.io operator: Equal value: frontend nodeSelector: kubernetes.io/os: linux node-role.deckhouse.io/frontend: \"\" ... Заполнение параметров подключения к приватному registry для установки в закрытом контуре без доступа в интернет Для подключения к приватному registry необходимо: Задать адрес и путь для параметра openelb.global.imageRegistry . Указать наименование секрета с правами доступа к приватному registry в параметре imagePullSecrets . Секрет должен быть создан вручную и зашифрован в Base64. ## Настройки openelb openelb: ## параметры подключения к приватному registry global: ## адрес и путь для приватного registry imageRegistry: registry.example.com/docker/addons ## секрет с правами доступа к приватному registry должен быть создан вручную, зашифрованный в Base64 imagePullSecrets: - name: myRegistryKeySecretName где формат imageRegistry : адрес — registry.example.com . Установите OpenELB в Kubernetes. Для установки через интернет: helm upgrade --install openelb elma365/openelb -f values-openelb.yaml -n openelb-system --create-namespace Для установки без доступа в интернет Перейдите в каталог с загруженным чартом и выполните команду: helm upgrade --install openelb ./openelb -f values-openelb.yaml -n openelb-system --create-namespace Настройте отказоустойчивость openelb-controller. Для достижения отказоустойчивости увеличьте количество реплик openelb-controller, выполнив команду на ноде master 1: kubectl scale --replicas=2 deployment openelb-controller -n openelb-system Выполните следующую команду, чтобы проверить находится ли openelb-controller в состоянии READY: 1/1 и STATUS: Running . Если да, OpenELB успешно установлен. kubectl get po -n openelb-system Создайте пул IP-адресов для OpenELB. Создайте на ноде master 1 файл vip-eip.yaml с описанием EIP‑объекта. Объект Eip функционирует как пул IP-адресов для OpenELB. Пример файла vip-eip.yaml : apiVersion: network.kubesphere.io/v1alpha2 kind: Eip metadata: name: vip-eip annotations: eip.openelb.kubesphere.io/is-default-eip: \"true\" spec: address: 192.168.1.13 protocol: vip interface: ens18 Примените файл vip-eip.yaml в Kubernetes, выполнив команду: kubectl apply -f vip-eip.yaml Переместите поды keepalived на frontend-узлы. По умолчанию поды keepalived размещаются openelb-manager на worker-узлах. В рамках статьи поды keepalived нужно разместить на frontend‑узлах. Внесите изменения в DaemonSet openelb-keepalive-vip , выполнив команду на ноде master 1: kubectl patch ds -n openelb-system openelb-keepalive-vip -p '{\"spec\":{\"template\":{\"spec\":{\"nodeSelector\":{\"kubernetes.io/os\":\"linux\",\"node-role.deckhouse.io/frontend\":\"\"},\"tolerations\":[{\"key\":\"dedicated.deckhouse.io\",\"value\":\"frontend\",\"effect\":\"NoExecute\"}]}}}}' Проверьте, что изменения, внесённые в DaemonSet openelb-keepalive-vip применились и поды разместились на нодах frontend: kubectl get po -o wide -n openelb-system Шаг 12. Добавление Ingress Nginx Controller - LoadBalancer Deckhouse устанавливает и управляет NGINX Ingress Controller при помощи Custom Resources. Если узлов для размещения Ingress-контроллера больше одного, он устанавливается в отказоустойчивом режиме и учитывает все особенности реализации инфраструктуры облаков и bare metal, а также кластеров Kubernetes различных типов. Создайте на ноде master 1 файл ingress-nginx-controller.yml , содержащий конфигурацию Ingress-контроллера. Пример файла ingress-nginx-controller.yml # секция, описывающая параметры nginx ingress controller # используемая версия API Deckhouse apiVersion: deckhouse.io/v1 kind: IngressNginxController metadata: name: nginx spec: # имя Ingress-класса для обслуживания Ingress NGINX controller ingressClass: nginx # способ поступления трафика из внешнего мира inlet: LoadBalancer # Аннотация для OpenELB в сервис nginx-load-balancer loadBalancer: annotations: eip.openelb.kubesphere.io/v1alpha2: \"vip-eip\" lb.kubesphere.io/v1alpha1: \"openelb\" hostPort: httpPort: 80 httpsPort: 443 # описывает, на каких узлах будет находиться компонент nodeSelector: node-role.kubernetes.io/frontend: \"\" tolerations: - operator: Exists Примените файл ingress-nginx-controller.yml в Kubernetes, выполнив команду: kubectl create -f ingress-nginx-controller.yml После установки ingress-контроллера Deckhouse автоматически создаст сервис nginx-load-balancer в namespace d8-ingress-nginx , но не свяжет данный сервис с OpenELB. Проверьте, что изменения, внесённые в сервис nginx-load-balancer применились, в EXTERNAL-IP появился IP-адрес 192.168.1.13. Для этого выполните следующую команду: kubectl get svc -n d8-ingress-nginx Шаг 13. Добавление пользователя для доступа к веб-интерфейсу кластера Создайте на ноде master 1 файл user.yml , содержащий описание учётной записи пользователя и прав доступа. Пример файла user.yml apiVersion: deckhouse.io/v1 kind: ClusterAuthorizationRule metadata: name: admin spec: # список учётных записей Kubernetes RBAC subjects: - kind: User name: admin@deckhouse.io # предустановленный шаблон уровня доступа accessLevel: SuperAdmin # разрешить пользователю делать kubectl port-forward portForwarding: true --- # секция, описывающая параметры статического пользователя # используемая версия API Deckhouse apiVersion: deckhouse.io/v1 kind: User metadata: name: admin spec: # e-mail пользователя email: admin@deckhouse.io # это хэш пароля xgnv5gkggd, сгенерированного сейчас # сгенерируйте свой или используйте этот, но только для тестирования # echo \"xgnv5gkggd\" | htpasswd -BinC 10 \"\" | cut -d: -f2 # возможно, захотите изменить password: '$2a$10$4j4cUeyonCfX7aDJyqSHXuAxycsf/sDK0T4n9ySQ7.owE34L1uXTm' Примените файл user.yml в Kubernetes, выполнив команду: kubectl create -f user.yml Шаг 14. Привилегии запускаемых нагрузок Разрешите переназначить политику привилегий для запускаемых подов: kubectl label namespace elma365 security.deckhouse.io/pod-policy=privileged --overwrite kubernetes-deckhouse-air-gap.html embedded-databases-settings.html Была ли статья полезной? Спасибо за ваш отзыв! × Ваш отзыв успешно отправлен! Спасибо за обратную связь. × Уточните, почему: Рекомендации не помогли Текст трудно понять Нет ответа на мой вопрос Содержание статьи не соответствует заголовку Другая причина × Расскажите, что вам не понравилось в статье: Да Нет Выберите вариант Рекомендации не помогли Текст трудно понять Нет ответа на мой вопрос Содержание статьи не соответствует заголовку Другая причина В этой статье API TS SDK Community Академия Платформа ECM Service Проекты Отправить фидбэк Задать вопрос Ваш фидбэк отправлен. © 2025 ELMA365 © 2025 ELMA365 elma365.com",
  "links": [
    "https://elma365.com/ru/help",
    "https://elma365.com/ru/help/platform/get-trial.html",
    "https://elma365.com/ru/help/ecm/ecm-functions.html",
    "https://elma365.com/ru/help/crm/crm-functions.html",
    "https://elma365.com/ru/help/projects/projects-functions.html",
    "https://elma365.com/ru/help/service/service-functions.html",
    "https://elma365.com/ru/help/kedo/kedo.html",
    "https://elma365.com/ru/help/business_solutions/-elma365-store.html",
    "https://elma-academy.com/ru/",
    "https://community.elma365.com/ru/",
    "https://api.elma365.com/ru/",
    "https://tssdk.elma365.com/latest/",
    "https://elma365.com/ru/",
    "https://elma365.com/ru/help/business_solutions/ /ru/help/platform/elma365-on-premises.html",
    "https://elma365.com/ru/help/business_solutions/ /ru/help/platform/infrastructure-preparation.html",
    "https://deckhouse.ru/",
    "https://reestr.digital.gov.ru/reestr/490559/",
    "https://landscape.cncf.io/",
    "https://elma365.com/ru/help/business_solutions/ /ru/help/platform/fail-safe-kubernetes-cluster.html",
    "https://deckhouse.ru/documentation/latest/modules/040-control-plane-manager/",
    "https://elma365.com/ru/help/business_solutions/ /ru/help/platform/downloading-images-deckhouse.html",
    "https://deckhouse.ru/documentation/v1/supported_versions.html",
    "https://deckhouse.ru/gs/",
    "https://deckhouse.ru/documentation/v1/deckhouse-faq.html",
    "https://github.com/deckhouse/deckhouse/tree/main/dhctl/",
    "https://deckhouse.ru/documentation/latest/modules/040-node-manager/cr.html",
    "https://github.com/helm/helm/releases",
    "https://openelb.io/docs/getting-started/installation/",
    "https://elma365.com/ru/help/business_solutions/ /ru/help/platform/kubernetes-deckhouse-air-gap.html",
    "https://elma365.com/ru/help/business_solutions/ /ru/help/platform/embedded-databases-settings.html",
    "https://tssdk.elma365.com/",
    "https://community.elma365.com/",
    "https://elma-academy.com/ru/elma365",
    "https://navigator.sk.ru/orn/1122971",
    "https://vkvideo.ru/@elma_bpm",
    "https://vk.com/elma_bpm",
    "https://t.me/elmaday",
    "https://dzen.ru/elma"
  ],
  "last_crawled": "2025-11-29T22:03:26.659717",
  "metadata": {
    "depth": 10,
    "saved_at": "2025-11-29T22:03:26.712559"
  }
}